import sqlite3
import re
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DB_PATH = 'c:/Users/haenee/mkt_ai_agent/backend/src/database/db.sqlite'

# ê³µí†µ ì •ì œ íŒ¨í„´ë“¤
NEWS_PATTERNS = [
    r'\(ì„œìš¸=ë‰´ìŠ¤1\)',
    r'\(ë¬´ë‹¨ì „ì¬\)',
    r'\(ì¢…í•©\)',
    r'\(ê¸°ì‚¬\)',
    r'\(ì‚¬ì§„\)',
    r'\(ì˜ìƒ\)',
    r'\(ì¸í„°ë·°\)',
    r'\(ë‹¨ë…\)',
    r'\(ì†ë³´\)',
    r'\(ì—…ë°ì´íŠ¸\)',
    r'\(ìˆ˜ì •\)',
    r'\(ì¶”ê°€\)',
]

SPECIAL_CHARS = r'[â˜…â˜†â—†â—‡â– â–¡â—â—‹â—â€»â†’â†â†‘â†“â†”â‡’â‡â‡‘â‡“â‡”]'

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
    return sqlite3.connect(DB_PATH)

def clean_whitespace(text):
    """ì—°ì†ëœ ê³µë°± ì •ë¦¬"""
    if not text:
        return text
    return re.sub(r'\s+', ' ', text).strip()

def update_article_content(conn, article_id, content, cleaned_content, operation_name):
    """ê¸°ì‚¬ ë‚´ìš© ì—…ë°ì´íŠ¸ ê³µí†µ í•¨ìˆ˜"""
    if cleaned_content != content:
        conn.cursor().execute("UPDATE articles SET content = ? WHERE id = ?", (cleaned_content, article_id))
        logger.info(f"{operation_name} ì™„ë£Œ (ID: {article_id})")
        return True
    return False

def clean_nbsp_content():
    """content ì¹¼ëŸ¼ì—ì„œ NBSP ì œê±°"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # NBSPê°€ í¬í•¨ëœ ê¸°ì‚¬ ìˆ˜ í™•ì¸
    c.execute("SELECT COUNT(*) FROM articles WHERE content LIKE '%&nbsp;%' OR content LIKE '%\xa0%'")
    nbsp_count = c.fetchone()[0]
    logger.info(f"NBSPê°€ í¬í•¨ëœ ê¸°ì‚¬ ìˆ˜: {nbsp_count}ê°œ")
    
    if nbsp_count > 0:
        # NBSP ì œê±°
        c.execute("""
            UPDATE articles 
            SET content = REPLACE(REPLACE(content, '&nbsp;', ' '), '\xa0', ' ')
            WHERE content LIKE '%&nbsp;%' OR content LIKE '%\xa0%'
        """)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        c.execute("""
            UPDATE articles 
            SET content = TRIM(REPLACE(REPLACE(REPLACE(content, '  ', ' '), '  ', ' '), '  ', ' '))
            WHERE content LIKE '%  %'
        """)
        
        conn.commit()
        logger.info("NBSP ì œê±° ì™„ë£Œ")
    
    conn.close()

def clean_title_zwnbsp():
    """title ì¹¼ëŸ¼ì—ì„œ ZWNBSP ì œê±°"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # ZWNBSPê°€ í¬í•¨ëœ ì œëª© ìˆ˜ í™•ì¸
    c.execute("SELECT COUNT(*) FROM articles WHERE title LIKE '%\uFEFF%'")
    zwnbsp_count = c.fetchone()[0]
    logger.info(f"ZWNBSPê°€ í¬í•¨ëœ ì œëª© ìˆ˜: {zwnbsp_count}ê°œ")
    
    if zwnbsp_count > 0:
        # ZWNBSP ì œê±°
        c.execute("""
            UPDATE articles 
            SET title = REPLACE(title, '\uFEFF', '')
            WHERE title LIKE '%\uFEFF%'
        """)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        c.execute("""
            UPDATE articles 
            SET title = TRIM(REPLACE(REPLACE(REPLACE(title, '  ', ' '), '  ', ' '), '  ', ' '))
            WHERE title LIKE '%  %'
        """)
        
        conn.commit()
        logger.info("title ZWNBSP ì œê±° ì™„ë£Œ")
    
    conn.close()

def clean_press_domain():
    """press ì¹¼ëŸ¼ì—ì„œ ë„ë©”ì¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ì œ"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # í˜„ì¬ press ë°ì´í„° í™•ì¸
    c.execute("SELECT DISTINCT press FROM articles WHERE press IS NOT NULL AND press != ''")
    current_presses = [row[0] for row in c.fetchall()]
    logger.info(f"í˜„ì¬ ì–¸ë¡ ì‚¬ ëª©ë¡: {current_presses}")
    
    # ë„ë©”ì¸ ì •ì œ í•¨ìˆ˜
    def extract_domain(url):
        if not url:
            return ""
        
        # www. ì œê±°
        domain = url.replace('www.', '')
        
        # .co.kr, .com, .net ë“± ì œê±°
        domain = re.sub(r'\.(co\.kr|com|net|org|kr)$', '', domain)
        
        # ì¶”ê°€ ë„ë©”ì¸ í™•ì¥ì ì œê±°
        domain = re.sub(r'\.(info|biz|edu|gov|mil|int)$', '', domain)
        
        return domain.strip()
    
    # ê° ê¸°ì‚¬ë³„ë¡œ press ì •ì œ
    c.execute("SELECT id, press FROM articles WHERE press IS NOT NULL AND press != ''")
    articles = c.fetchall()
    
    updated_count = 0
    for article_id, press in articles:
        cleaned_press = extract_domain(press)
        if cleaned_press != press:
            c.execute("UPDATE articles SET press = ? WHERE id = ?", (cleaned_press, article_id))
            updated_count += 1
            logger.info(f"ì–¸ë¡ ì‚¬ ì •ì œ: {press} -> {cleaned_press}")
    
    conn.commit()
    logger.info(f"ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì •ì œ ì™„ë£Œ: {updated_count}ê°œ ì—…ë°ì´íŠ¸")
    
    # ì •ì œ í›„ ì–¸ë¡ ì‚¬ ëª©ë¡ í™•ì¸
    c.execute("SELECT DISTINCT press FROM articles WHERE press IS NOT NULL AND press != '' ORDER BY press")
    cleaned_presses = [row[0] for row in c.fetchall()]
    logger.info(f"ì •ì œ í›„ ì–¸ë¡ ì‚¬ ëª©ë¡: {cleaned_presses}")
    
    conn.close()

def convert_date_format():
    """pub_dateì™€ created_at ì¹¼ëŸ¼ì„ yyyy-mm-dd HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # pub_date ë³€í™˜
    c.execute("SELECT id, pub_date FROM articles WHERE pub_date IS NOT NULL AND pub_date != ''")
    pub_dates = c.fetchall()
    
    updated_pub_count = 0
    for article_id, pub_date in pub_dates:
        try:
            if pub_date and isinstance(pub_date, str):
                # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸ (yyyy-mm-dd HH:MM:SS)
                if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$', pub_date):
                    continue  # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ë©´ ê±´ë„ˆë›°ê¸°
                
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                date_formats = [
                    "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
                    "%Y-%m-%d %H:%M:%S",         # ISO í˜•ì‹
                    "%Y-%m-%d",                  # ë‚ ì§œë§Œ
                    "%d %b %Y %H:%M:%S",         # ì‹œê°„ëŒ€ ì—†ìŒ
                ]
                
                parsed_date = None
                for fmt in date_formats:
                    try:
                        parsed_date = datetime.strptime(pub_date, fmt)
                        break
                    except ValueError:
                        continue
                
                if parsed_date:
                    formatted_date = parsed_date.strftime("%Y-%m-%d %H:%M:%S")
                    c.execute("UPDATE articles SET pub_date = ? WHERE id = ?", (formatted_date, article_id))
                    updated_pub_count += 1
                    logger.info(f"pub_date ë³€í™˜: {pub_date} -> {formatted_date}")
                else:
                    logger.warning(f"ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨: {pub_date}")
                    
        except Exception as e:
            logger.error(f"pub_date ë³€í™˜ ì˜¤ë¥˜ (ID: {article_id}): {e}")
    
    # created_at ë³€í™˜ (SQLite datetimeì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ)
    c.execute("""
        UPDATE articles 
        SET created_at = strftime('%Y-%m-%d %H:%M:%S', created_at)
        WHERE created_at IS NOT NULL
    """)
    
    conn.commit()
    logger.info(f"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì™„ë£Œ: pub_date {updated_pub_count}ê°œ ì—…ë°ì´íŠ¸")
    
    conn.close()

def clean_classification_logs_dates():
    """classification_logs í…Œì´ë¸”ì˜ created_at ì¹¼ëŸ¼ì„ yyyy-mm-dd HH:MM:SS í˜•ì‹ìœ¼ë¡œ ì •ì œ"""
    conn = get_db_connection()
    c = conn.cursor()
    
    try:
        # classification_logs í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        c.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='classification_logs'
        """)
        
        if not c.fetchone():
            logger.warning("classification_logs í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        # í˜„ì¬ created_at ë°ì´í„° ìƒ˜í”Œ í™•ì¸
        c.execute("""
            SELECT created_at, COUNT(*) as count 
            FROM classification_logs 
            WHERE created_at IS NOT NULL 
            GROUP BY created_at 
            ORDER BY count DESC 
            LIMIT 10
        """)
        
        current_formats = c.fetchall()
        logger.info("í˜„ì¬ created_at í˜•ì‹ ìƒ˜í”Œ:")
        for date_format, count in current_formats:
            logger.info(f"  {date_format}: {count}ê°œ")
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        date_formats = [
            "%Y-%m-%dT%H:%M:%S.%f",  # 2025-07-23T10:54:29.732630
            "%Y-%m-%d %H:%M:%S",     # 2025-07-21 15:25:53
            "%Y-%m-%dT%H:%M:%S",     # 2025-07-23T10:54:29
            "%Y-%m-%d",              # ë‚ ì§œë§Œ
            "%d/%m/%Y %H:%M:%S",     # ë‹¤ë¥¸ í˜•ì‹ë“¤
            "%m/%d/%Y %H:%M:%S",
        ]
        
        # ëª¨ë“  created_at ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        c.execute("SELECT rowid, created_at FROM classification_logs WHERE created_at IS NOT NULL")
        all_dates = c.fetchall()
        
        updated_count = 0
        failed_count = 0
        
        for rowid, date_str in all_dates:
            try:
                if not date_str or date_str == '':
                    continue
                
                # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
                if re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$', date_str):
                    continue
                
                # ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„
                parsed_date = None
                for fmt in date_formats:
                    try:
                        parsed_date = datetime.strptime(date_str, fmt)
                        break
                    except ValueError:
                        continue
                
                if parsed_date:
                    # í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    formatted_date = parsed_date.strftime("%Y-%m-%d %H:%M:%S")
                    
                    # ì—…ë°ì´íŠ¸
                    c.execute("""
                        UPDATE classification_logs 
                        SET created_at = ? 
                        WHERE rowid = ?
                    """, (formatted_date, rowid))
                    
                    updated_count += 1
                    logger.info(f"ë‚ ì§œ ë³€í™˜: {date_str} -> {formatted_date}")
                else:
                    failed_count += 1
                    logger.warning(f"ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜ (rowid: {rowid}): {e}")
        
        conn.commit()
        
        logger.info(f"classification_logs ë‚ ì§œ ì •ì œ ì™„ë£Œ:")
        logger.info(f"  - ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ: {updated_count}ê°œ")
        logger.info(f"  - ì‹¤íŒ¨í•œ ë ˆì½”ë“œ: {failed_count}ê°œ")
        
        # ì •ì œ í›„ ê²°ê³¼ í™•ì¸
        c.execute("""
            SELECT created_at, COUNT(*) as count 
            FROM classification_logs 
            WHERE created_at IS NOT NULL 
            GROUP BY created_at 
            ORDER BY count DESC 
            LIMIT 5
        """)
        
        cleaned_formats = c.fetchall()
        logger.info("ì •ì œ í›„ created_at í˜•ì‹ ìƒ˜í”Œ:")
        for date_format, count in cleaned_formats:
            logger.info(f"  {date_format}: {count}ê°œ")
        
    except Exception as e:
        logger.error(f"classification_logs ë‚ ì§œ ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        conn.rollback()
    finally:
        conn.close()

def clean_classification_logs_data():
    """classification_logs í…Œì´ë¸” ì „ì²´ ë°ì´í„° ì •ì œ"""
    conn = get_db_connection()
    c = conn.cursor()
    
    try:
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        c.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='classification_logs'
        """)
        
        if not c.fetchone():
            logger.warning("classification_logs í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        # í…Œì´ë¸” í˜„í™© í™•ì¸
        c.execute("SELECT COUNT(*) FROM classification_logs")
        total_count = c.fetchone()[0]
        logger.info(f"classification_logs í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {total_count}ê°œ")
        
        # NULL ê°’ ì²˜ë¦¬
        c.execute("""
            UPDATE classification_logs 
            SET confidence_score = 0 
            WHERE confidence_score IS NULL
        """)
        null_updated = c.rowcount
        logger.info(f"NULL confidence_scoreë¥¼ 0ìœ¼ë¡œ ë³€ê²½: {null_updated}ê°œ")
        
        # is_saved ì¹¼ëŸ¼ ì •ì œ
        c.execute("""
            UPDATE classification_logs 
            SET is_saved = 1 
            WHERE is_saved IS NULL OR is_saved = 0
        """)
        saved_updated = c.rowcount
        logger.info(f"is_savedë¥¼ 1ë¡œ ì„¤ì •: {saved_updated}ê°œ")
        
        # ì¤‘ë³µ URL ì œê±° (ê°€ì¥ ìµœê·¼ ê²ƒë§Œ ìœ ì§€)
        c.execute("""
            DELETE FROM classification_logs 
            WHERE rowid NOT IN (
                SELECT MAX(rowid) 
                FROM classification_logs 
                GROUP BY url
            )
        """)
        duplicate_removed = c.rowcount
        logger.info(f"ì¤‘ë³µ URL ì œê±°: {duplicate_removed}ê°œ")
        
        conn.commit()
        
        # ì •ì œ í›„ í˜„í™©
        c.execute("SELECT COUNT(*) FROM classification_logs")
        final_count = c.fetchone()[0]
        logger.info(f"ì •ì œ í›„ ì´ ë ˆì½”ë“œ: {final_count}ê°œ")
        
    except Exception as e:
        logger.error(f"classification_logs ë°ì´í„° ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        conn.rollback()
    finally:
        conn.close()

def remove_duplicate_articles():
    """ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (URL ê¸°ì¤€)"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # ì¤‘ë³µ URL í™•ì¸
    c.execute("""
        SELECT url, COUNT(*) as count 
        FROM articles 
        WHERE url IS NOT NULL AND url != ''
        GROUP BY url 
        HAVING COUNT(*) > 1
    """)
    duplicates = c.fetchall()
    
    if duplicates:
        logger.info(f"ì¤‘ë³µ URL ë°œê²¬: {len(duplicates)}ê°œ")
        
        for url, count in duplicates:
            logger.info(f"ì¤‘ë³µ URL: {url} ({count}ê°œ)")
            
            # ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
            c.execute("""
                DELETE FROM articles 
                WHERE url = ? AND id NOT IN (
                    SELECT MAX(id) FROM articles WHERE url = ?
                )
            """, (url, url))
        
        conn.commit()
        logger.info("ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì™„ë£Œ")
    else:
        logger.info("ì¤‘ë³µ ê¸°ì‚¬ ì—†ìŒ")
    
    conn.close()

def clean_content_characters():
    """content ì¹¼ëŸ¼ì—ì„œ í•œìì™€ ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # í•œìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜
    def remove_chinese_and_special_chars(text):
        if not text:
            return text
        
        # í•œì ì œê±° (Unicode ë²”ìœ„: 4E00-9FFF)
        text = re.sub(r'[\u4e00-\u9fff]', '', text)
        
        # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(SPECIAL_CHARS, '', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        return clean_whitespace(text)
    
    # content ì •ì œ
    c.execute("SELECT id, content FROM articles WHERE content IS NOT NULL AND content != ''")
    articles = c.fetchall()
    
    updated_count = 0
    for article_id, content in articles:
        cleaned_content = remove_chinese_and_special_chars(content)
        if update_article_content(conn, article_id, content, cleaned_content, "ë¬¸ì ì •ì œ"):
            updated_count += 1
    
    conn.commit()
    logger.info(f"í•œì/íŠ¹ìˆ˜ë¬¸ì ì œê±° ì™„ë£Œ: {updated_count}ê°œ ì—…ë°ì´íŠ¸")
    
    conn.close()

def clean_news_patterns():
    """ë‰´ìŠ¤ ê´€ë ¨ íŒ¨í„´ ì œê±°"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # íŒ¨í„´ ì œê±° í•¨ìˆ˜
    def remove_news_patterns(text):
        if not text:
            return text
        
        for pattern in NEWS_PATTERNS:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        return clean_whitespace(text)
    
    # content ì •ì œ
    c.execute("SELECT id, content FROM articles WHERE content IS NOT NULL AND content != ''")
    articles = c.fetchall()
    
    updated_count = 0
    for article_id, content in articles:
        cleaned_content = remove_news_patterns(content)
        if update_article_content(conn, article_id, content, cleaned_content, "ë‰´ìŠ¤ íŒ¨í„´ ì œê±°"):
            updated_count += 1
    
    conn.commit()
    logger.info(f"ë‰´ìŠ¤ ê´€ë ¨ íŒ¨í„´ ì œê±° ì™„ë£Œ: {updated_count}ê°œ ì—…ë°ì´íŠ¸")
    
    conn.close()

def show_cleaning_summary():
    """ì •ì œ ì‘ì—… í›„ í†µê³„ ì¶œë ¥"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # ì „ì²´ ê¸°ì‚¬ ìˆ˜
    c.execute("SELECT COUNT(*) FROM articles")
    total_articles = c.fetchone()[0]
    
    # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜
    c.execute("SELECT keyword, COUNT(*) FROM articles GROUP BY keyword ORDER BY COUNT(*) DESC")
    keyword_stats = c.fetchall()
    
    # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜
    c.execute("SELECT press, COUNT(*) FROM articles WHERE press IS NOT NULL AND press != '' GROUP BY press ORDER BY COUNT(*) DESC LIMIT 10")
    press_stats = c.fetchall()
    
    logger.info("=== ë°ì´í„° ì •ì œ ì™„ë£Œ í†µê³„ ===")
    logger.info(f"ì „ì²´ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    logger.info("í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜:")
    for keyword, count in keyword_stats:
        logger.info(f"  {keyword}: {count}ê°œ")
    logger.info("ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬:")
    for press, count in press_stats:
        logger.info(f"  {press}: {count}ê°œ")
    
    conn.close()

def remove_english_only_articles():
    """ì œëª©ì´ ëª¨ë‘ ì˜ë¬¸ì¸ ê¸°ì‚¬ ì‚­ì œ (í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°)"""
    conn = get_db_connection()
    c = conn.cursor()
    
    # articles í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ í™•ì¸ ë° ì‚­ì œ
    logger.info("articles í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ í™•ì¸ ì¤‘...")
    
    c.execute("SELECT id, title FROM articles WHERE title IS NOT NULL AND title != ''")
    articles = c.fetchall()
    
    english_articles = []
    for article_id, title in articles:
        # í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ê³ , ì˜ë¬¸ì´ í¬í•¨ëœ ê²½ìš°
        if title and not re.search(r'[ê°€-í£]', title) and re.search(r'[A-Za-z]', title):
            english_articles.append((article_id, title))
    
    logger.info(f"articles í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ë°œê²¬: {len(english_articles)}ê°œ")
    
    if english_articles:
        for article_id, title in english_articles:
            logger.info(f"ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ (ID: {article_id}): {title[:50]}...")
            c.execute("DELETE FROM articles WHERE id = ?", (article_id,))
        
        conn.commit()
        logger.info(f"articles í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ {len(english_articles)}ê°œ ì‚­ì œ ì™„ë£Œ")
    
    # classification_logs í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ í™•ì¸ ë° ì‚­ì œ
    try:
        c.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='classification_logs'
        """)
        
        if c.fetchone():
            logger.info("classification_logs í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ í™•ì¸ ì¤‘...")
            
            c.execute("SELECT id, title FROM classification_logs WHERE title IS NOT NULL AND title != ''")
            classification_logs = c.fetchall()
            
            english_classification_logs = []
            for log_id, title in classification_logs:
                # í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ê³ , ì˜ë¬¸ì´ í¬í•¨ëœ ê²½ìš°
                if title and not re.search(r'[ê°€-í£]', title) and re.search(r'[A-Za-z]', title):
                    english_classification_logs.append((log_id, title))
            
            logger.info(f"classification_logs í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ë°œê²¬: {len(english_classification_logs)}ê°œ")
            
            if english_classification_logs:
                for log_id, title in english_classification_logs:
                    logger.info(f"ì˜ë¬¸ ì œëª© ë¶„ë¥˜ ë¡œê·¸ ì‚­ì œ (ID: {log_id}): {title[:50]}...")
                    c.execute("DELETE FROM classification_logs WHERE id = ?", (log_id,))
                
                conn.commit()
                logger.info(f"classification_logs í…Œì´ë¸”ì—ì„œ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ {len(english_classification_logs)}ê°œ ì‚­ì œ ì™„ë£Œ")
        else:
            logger.info("classification_logs í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        logger.error(f"classification_logs í…Œì´ë¸” ì˜ë¬¸ ê¸°ì‚¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    conn.close()
    
    total_deleted = len(english_articles) + (len(english_classification_logs) if 'english_classification_logs' in locals() else 0)
    logger.info(f"ì´ ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ ì™„ë£Œ: {total_deleted}ê°œ")

def main():
    """ë°ì´í„° ì •ì œ ì‘ì—… ì‹¤í–‰"""
    logger.info("ë°ì´í„° ì •ì œ ì‘ì—… ì‹œì‘")
    
    print("\nğŸ”§ ì •ì œ ì‘ì—… ì„ íƒ:")
    print("1. articles í…Œì´ë¸” ì „ì²´ ì •ì œ")
    print("2. classification_logs í…Œì´ë¸” ë‚ ì§œ ì •ì œ")
    print("3. classification_logs í…Œì´ë¸” ì „ì²´ ì •ì œ")
    print("4. ëª¨ë“  í…Œì´ë¸” ì •ì œ")
    print("5. ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ")
    
    choice = input("\nğŸ¤” ì„ íƒí•˜ì„¸ìš” (1-5): ").strip()
    
    try:
        if choice == "1":
            # articles í…Œì´ë¸”ë§Œ ì •ì œ
            logger.info("=== articles í…Œì´ë¸” ì •ì œ ì‹œì‘ ===")
            
            # 1. NBSP ì œê±°
            logger.info("1. NBSP ì œê±° ì¤‘...")
            clean_nbsp_content()
            
            # 2. title ZWNBSP ì œê±°
            logger.info("2. title ZWNBSP ì œê±° ì¤‘...")
            clean_title_zwnbsp()
            
            # 3. ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì •ì œ
            logger.info("3. ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì •ì œ ì¤‘...")
            clean_press_domain()
            
            # 4. ë‚ ì§œ í˜•ì‹ ë³€í™˜
            logger.info("4. ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘...")
            convert_date_format()
            
            # 5. ì¤‘ë³µ ê¸°ì‚¬ ì œê±°
            logger.info("5. ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")
            remove_duplicate_articles()
            
            # 6. ë¬¸ì ì •ì œ
            logger.info("6. ë¬¸ì ì •ì œ ì¤‘...")
            clean_content_characters()

            # 7. ë‰´ìŠ¤ íŒ¨í„´ ì œê±°
            logger.info("7. ë‰´ìŠ¤ íŒ¨í„´ ì œê±° ì¤‘...")
            clean_news_patterns()
            
            # 8. ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ
            logger.info("8. ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ ì¤‘...")
            remove_english_only_articles()
            
            # 9. ì •ì œ ê²°ê³¼ í†µê³„
            logger.info("9. ì •ì œ ê²°ê³¼ í™•ì¸ ì¤‘...")
            show_cleaning_summary()
            
        elif choice == "2":
            # classification_logs ë‚ ì§œë§Œ ì •ì œ
            logger.info("=== classification_logs ë‚ ì§œ ì •ì œ ì‹œì‘ ===")
            clean_classification_logs_dates()
            
        elif choice == "3":
            # classification_logs ì „ì²´ ì •ì œ
            logger.info("=== classification_logs ì „ì²´ ì •ì œ ì‹œì‘ ===")
            clean_classification_logs_data()
            clean_classification_logs_dates()
            
        elif choice == "4":
            # ëª¨ë“  í…Œì´ë¸” ì •ì œ
            logger.info("=== ëª¨ë“  í…Œì´ë¸” ì •ì œ ì‹œì‘ ===")
            
            # articles í…Œì´ë¸” ì •ì œ
            logger.info("ğŸ“° articles í…Œì´ë¸” ì •ì œ ì¤‘...")
            clean_nbsp_content()
            clean_title_zwnbsp()
            clean_press_domain()
            convert_date_format()
            remove_duplicate_articles()
            clean_content_characters()
            clean_news_patterns()
            remove_english_only_articles()
            show_cleaning_summary()
            
            # classification_logs í…Œì´ë¸” ì •ì œ
            logger.info("ğŸ“Š classification_logs í…Œì´ë¸” ì •ì œ ì¤‘...")
            clean_classification_logs_data()
            clean_classification_logs_dates()
            
        elif choice == "5":
            # ì˜ë¬¸ ì œëª© ê¸°ì‚¬ë§Œ ì‚­ì œ
            logger.info("=== ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì‚­ì œ ì‹œì‘ ===")
            remove_english_only_articles()
            
        else:
            logger.error("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        logger.info("âœ… ë°ì´í„° ì •ì œ ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì •ì œ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main() 