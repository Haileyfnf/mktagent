import os
import sqlite3
import pandas as pd
import logging
from news_classifier import NewsClassifier
from dotenv import load_dotenv
import json
from datetime import datetime
import webbrowser
import time

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedVerification:
    """
    í–¥ìƒëœ ìˆ˜ë™ ê²€ì¦ ì‹œìŠ¤í…œ
    - ë¶„ë¥˜ ê¸°ì¤€ ê°€ì´ë“œ ì œê³µ
    - ë¹ ë¥¸ ê²€ì¦ ì˜µì…˜
    - ì§„í–‰ë¥  í‘œì‹œ
    - ì¼ì‹œ ì¤‘ë‹¨/ì¬ê°œ ê¸°ëŠ¥
    """
    
    def __init__(self, db_path=None, model_path="models"):
        self.db_path = db_path or os.getenv('DB_PATH')
        self.model_path = model_path
        self.classifier = NewsClassifier(db_path=db_path, model_path=model_path)
        
        # ë¸Œëœë“œ ì†Œìœ  ê´€ê³„ ì •ì˜ (ì§ì ‘ ì†Œìœ  ë¸Œëœë“œ + ìíšŒì‚¬)
        self.brand_ownership = {
            "F&F": {
                "brands": ["MLB", "ë””ìŠ¤ì»¤ë²„ë¦¬", "ë””ìŠ¤ì»¤ë²„ë¦¬ ìµìŠ¤í˜ë””ì…˜"],
                "subsidiaries": ["F&Fì—”í„°í…Œì¸ë¨¼íŠ¸", "F&Fì¸í„°ë‚´ì…”ë„", "F&Fí™€ë”©ìŠ¤"]
            },
            "ì´ëœë“œ": {
                "brands": ["ë‰´ë°œë€ìŠ¤", "íœ ë¼"],
                "subsidiaries": ["ì´ëœë“œì›”ë“œ", "ì´ëœë“œë¦¬í…Œì¼"]
            },
            "ì˜ì›ë¬´ì—­": {
                "brands": ["ë…¸ìŠ¤í˜ì´ìŠ¤"],
                "subsidiaries": []
            }
        }
        
        # ë¶„ë¥˜ ê¸°ì¤€ ì •ì˜
        self.classification_guide = {
            "ë³´ë„ìë£Œ": [
                "ê¸°ì—…ì´ë‚˜ ì¡°ì§ì—ì„œ ê³µì‹ì ìœ¼ë¡œ ë°œí‘œí•œ ë‚´ìš©",
                "ì–¸ë¡ ì‚¬ê°€ ê¸°ì—…ì˜ ë³´ë„ìë£Œë¥¼ ê·¸ëŒ€ë¡œ ê²Œì¬",
                "ê¸°ì—…ì˜ ì„±ê³¼, ì‹¤ì , ê³„íš ë“±ì„ í™ë³´í•˜ëŠ” ë‚´ìš©",
                "ê¸°ì—…ëª…ì´ ì œëª©ì— í¬í•¨ë˜ê³  í™ë³´ì„± ë‚´ìš©",
                "ë³¸ë¬¸ì— '~ê´€ê³„ìì— ì˜í•˜ë©´', 'ë¸Œëœë“œ ê´€ê³„ì ì¸¡ì€', 'OOì‚¬ ê´€ê³„ìëŠ”' ë“±ì˜ í‘œí˜„",
                "ê¸°ì—…ì´ ì†Œìœ í•œ ë¸Œëœë“œë“¤ì˜ í™œë™ì„ í™ë³´í•˜ëŠ” ë‚´ìš©",
                "ì‚¬íšŒê³µí—Œí™œë™, í™˜ê²½ë³´í˜¸í™œë™ ë“± ê¸°ì—…ì˜ ê¸ì •ì  í™œë™ í™ë³´",
                "âš ï¸ ê¸°ìê°€ ì—¬ëŸ¬ ë¸Œëœë“œì˜ ë³´ë„ìë£Œë¥¼ í•©ì³ì„œ í¸ì§‘í•œ ê¸°ì‚¬ë„ ë³´ë„ìë£Œ",
                "âš ï¸ ì œëª©ì— ì—¬ëŸ¬ ë¸Œëœë“œëª…ì´ ë‚˜ì—´ë˜ì–´ ìˆì–´ë„ ë³¸ë¬¸ì— ë³´ë„ìë£Œ íŠ¹ì„±ì´ ìˆìœ¼ë©´ ë³´ë„ìë£Œ",
                "âš ï¸ ë¸Œëœë“œ ì†Œìœ  ê´€ê³„: F&F(MLB, ë””ìŠ¤ì»¤ë²„ë¦¬ + F&Fì—”í„°í…Œì¸ë¨¼íŠ¸ ë“± ìíšŒì‚¬), ì´ëœë“œ(ë‰´ë°œë€ìŠ¤, íœ ë¼), ë…¸ìŠ¤í˜ì´ìŠ¤(ë…ë¦½)",
                "âš ï¸ ìˆ˜ì§‘ í‚¤ì›Œë“œì™€ ì‹¤ì œ ë³´ë„ìë£Œ ì£¼ì²´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: ë””ìŠ¤ì»¤ë²„ë¦¬ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘í–ˆì§€ë§Œ F&Fì˜ ë³´ë„ìë£Œ)",
                "ì˜ˆì‹œ: 'OOê¸°ì—…, ë§¤ì¶œ 20% ì¦ê°€ ë°œí‘œ', 'OOì‚¬ ì‹ ì œí’ˆ ì¶œì‹œ', 'F&F, MLBÂ·ë””ìŠ¤ì»¤ë²„ë¦¬ ë§¤ì¥ì— ì˜ë¥˜ìˆ˜ê±°í•¨ ì„¤ì¹˜', 'ì´ëœë“œ ë‰´ë°œë€ìŠ¤, 2025 ë©¤ë²„ìŠ¤ìœ„í¬ ìº í˜ì¸ ê°œìµœ'"
            ],
            "ì˜¤ê°€ë‹‰": [
                "ì–¸ë¡ ì‚¬ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ì„±í•œ ê¸°ì‚¬",
                "ê¸°ì—…ì´ë‚˜ ì¡°ì§ê³¼ ë¬´ê´€í•œ ê°ê´€ì  ë¶„ì„",
                "ì‹œì¥ ë™í–¥, íŠ¸ë Œë“œ ë¶„ì„ ë“±",
                "ê¸°ì—…ëª…ì´ ì–¸ê¸‰ë˜ì§€ë§Œ í™ë³´ì„±ì´ ì•„ë‹Œ ë‚´ìš©",
                "ì—¬ëŸ¬ ë¸Œëœë“œ í˜¹ì€ ê¸°ì—…ëª…ì´ ê°™ì´ ì–¸ê¸‰ë˜ëŠ” ë‚´ìš©",
                "âš ï¸ ì¤‘ìš”: í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ëŠ” í•´ë‹¹ ê¸°ì—… ê´€ë ¨ ë‚´ìš©ì´ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ",
                "âš ï¸ ì£¼ì‹/ê¸ˆìœµ ê´€ë ¨ ì œëª©ì´ë¼ë„ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ê¸°ì—… ë‚´ìš©ì´ ìˆìœ¼ë©´ 'ì˜¤ê°€ë‹‰'ì¼ ìˆ˜ ìˆìŒ",
                "âš ï¸ ì—°ì˜ˆì¸/ì¸í”Œë£¨ì–¸ì„œê°€ ë¸Œëœë“œ ì œí’ˆì„ ì°©ìš©í•˜ëŠ” ë¼ì´í”„ìŠ¤íƒ€ì¼/íŠ¸ë Œë“œ ê¸°ì‚¬",
                "ì˜ˆì‹œ: 'ITì—…ê³„ ë™í–¥ ë¶„ì„', 'ì‹œì¥ ë³€í™”ì— ë”°ë¥¸ OOê¸°ì—… ì˜í–¥', 'ìƒìŠ¹ë¥  ìƒìœ„ 50ì„  - ì½”ìŠ¤í”¼(ë³¸ë¬¸ì— F&F ì–¸ê¸‰)', 'ì°¨ì€ìš° ëŸ¬ë‹ ê¸°ì‚¬(ë…¸ìŠ¤í˜ì´ìŠ¤ ì œí’ˆ ì°©ìš©)'"
            ],
            "í•´ë‹¹ì—†ìŒ": [
                "í‚¤ì›Œë“œì™€ ê´€ë ¨ì´ ì—†ëŠ” ë‚´ìš©",
                "ë‹¨ìˆœíˆ í‚¤ì›Œë“œë§Œ ì–¸ê¸‰ë˜ê³  ì‹¤ì œ ë‚´ìš©ì€ ë‹¤ë¥¸ ì£¼ì œ",
                "ìŠ¤íŒ¸ì„±ì´ë‚˜ ê´‘ê³ ì„± ë‚´ìš©",
                "ë³¸ë¬¸ì„ í™•ì¸í•´ë„ í‚¤ì›Œë“œ ê¸°ì—…ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©",
                "âš ï¸ ë‹¤ë¥¸ ê¸°ì—…ì˜ ì´ë²¤íŠ¸/í• ì¸í–‰ì‚¬ì—ì„œ í‚¤ì›Œë“œ ë¸Œëœë“œê°€ ì°¸ì—¬í•˜ëŠ” ê²½ìš°",
                "âš ï¸ í‚¤ì›Œë“œ ë¸Œëœë“œê°€ ì–¸ê¸‰ë˜ì§€ë§Œ ì‹¤ì œ ì£¼ì²´ëŠ” ë‹¤ë¥¸ ê¸°ì—…ì¸ ê²½ìš°",
                "âš ï¸ ì œëª©ì€ ì¼ë°˜ì ì´ì§€ë§Œ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ëœ ê²½ìš° (ì¬ê²€í†  í•„ìš”)",
                "âš ï¸ ì—°ì˜ˆì¸/ì¸í”Œë£¨ì–¸ì„œê°€ ë¸Œëœë“œ ì œí’ˆì„ ì°©ìš©í•˜ëŠ” ê²½ìš°ëŠ” 'ì˜¤ê°€ë‹‰'ìœ¼ë¡œ ë¶„ë¥˜",
                "ì˜ˆì‹œ: 'OOê¸°ì—… ì£¼ì‹ íˆ¬ì íŒ', 'OOì‚¬ ê´€ë ¨ ë£¨ë¨¸', 'ì¿ íŒ¡ í‹°ì…”ì¸  í˜ì–´ì— ë‰´ë°œë€ìŠ¤ ì°¸ì—¬', '[ê²½ì œê³„ ì¸ì‚¬] ë‰´ë°œë€ìŠ¤ì½”ë¦¬ì•„ ëŒ€í‘œ (ë³¸ë¬¸ì— ë³´ë„ìë£Œ íŠ¹ì„±)', 'ì°¨ì€ìš° ëŸ¬ë‹ ê¸°ì‚¬ (ë…¸ìŠ¤í˜ì´ìŠ¤ ì œí’ˆ ì°©ìš©)'"
            ]
        }
    
    def show_classification_guide(self):
        """ë¶„ë¥˜ ê¸°ì¤€ ê°€ì´ë“œ í‘œì‹œ"""
        print("\n" + "="*80)
        print("ğŸ“‹ ë¶„ë¥˜ ê¸°ì¤€ ê°€ì´ë“œ")
        print("="*80)
        
        for category, criteria in self.classification_guide.items():
            print(f"\nğŸ”¸ {category}:")
            for i, criterion in enumerate(criteria, 1):
                print(f"   {i}. {criterion}")
        
        print("\n" + "="*80)
        input("ê°€ì´ë“œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. Enterë¥¼ ëˆŒëŸ¬ ê³„ì†í•˜ì„¸ìš”...")
    
    def predict_and_verify_enhanced(self, limit=20, auto_open_url=False):
        """
        í–¥ìƒëœ ì˜ˆì¸¡ ë° ê²€ì¦ ì‹œìŠ¤í…œ
        
        Args:
            limit: ê²€ì¦í•  ë°ì´í„° ê°œìˆ˜
            auto_open_url: URL ìë™ ì—´ê¸° ì—¬ë¶€
        """
        try:
            logger.info("ğŸ” í–¥ìƒëœ ì˜ˆì¸¡ ë° ê²€ì¦ ì‹œì‘...")
            
            # ë¶„ë¥˜ ê¸°ì¤€ ê°€ì´ë“œ í‘œì‹œ
            self.show_classification_guide()
            
            # ëª¨ë¸ ë¡œë“œ
            if not self.classifier.load_koelectra_model():
                logger.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
                return
            
            # ë¯¸ë¶„ë¥˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (group_nameë³„ ìµœì‹ ìˆœ)
            conn = sqlite3.connect(self.db_path)

            # 1ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ group_nameë“¤ ì¡°íšŒ
            group_query = """
            SELECT DISTINCT a.group_name, COUNT(*) as count
            FROM articles a
            LEFT JOIN classification_logs cl ON a.url = cl.url
            WHERE cl.url IS NULL
            GROUP BY a.group_name
            ORDER BY count DESC
            """
            group_df = pd.read_sql_query(group_query, conn)

            if len(group_df) == 0:
                logger.warning("ê²€ì¦í•  ë¯¸ë¶„ë¥˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                conn.close()
                return

            print(f"\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ê·¸ë£¹ í˜„í™©:")
            for _, row in group_df.iterrows():
                print(f"  {row['group_name']}: {row['count']}ê°œ")

            # 2ë‹¨ê³„: ê° group_nameë³„ë¡œ ìµœì‹ ìˆœìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë°ì´í„° ìˆ˜ì§‘
            articles_per_group = max(1, limit // len(group_df))  # ê·¸ë£¹ë‹¹ ìµœì†Œ 1ê°œ
            remaining = limit % len(group_df)  # ë‚¨ì€ ê°œìˆ˜

            all_articles = []

            for idx, group_row in group_df.iterrows():
                group_name = group_row['group_name']
                current_limit = articles_per_group + (1 if idx < remaining else 0)

                # ëª¨ë“  ê·¸ë£¹ì—ì„œ ê· í˜•ì¡íŒ ì„ íƒ (F&F íŠ¹ë³„ í‚¤ì›Œë“œëŠ” ê²€ì¦ ê³¼ì •ì—ì„œë§Œ ì•ˆë‚´)
                article_query = """
                SELECT a.id, a.title, a.content, a.keyword, a.group_name, a.created_at, a.url
                FROM articles a
                LEFT JOIN classification_logs cl ON a.url = cl.url
                WHERE cl.url IS NULL AND a.group_name = ?
                ORDER BY RANDOM()
                LIMIT ?
                """

                group_articles = pd.read_sql_query(article_query, conn, params=[group_name, current_limit])
                all_articles.append(group_articles)

                print(f"  {group_name}: {len(group_articles)}ê°œ ì„ íƒ")

            conn.close()

            # ëª¨ë“  ê·¸ë£¹ì˜ ë°ì´í„° í•©ì¹˜ê¸°
            df = pd.concat(all_articles, ignore_index=True)

            print(f"\nâœ… ì´ {len(df)}ê°œ ê¸°ì‚¬ë¥¼ {len(group_df)}ê°œ ê·¸ë£¹ì—ì„œ ìµœì‹ ìˆœìœ¼ë¡œ ê· ë“±í•˜ê²Œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")

            if len(df) == 0:
                logger.warning("ê²€ì¦í•  ë¯¸ë¶„ë¥˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ê²€ì¦ ê²°ê³¼ ì €ì¥ìš©
            verification_results = []
            session_file = os.path.join(self.model_path, "verification_session.json")
            
            # ì´ì „ ì„¸ì…˜ ë³µêµ¬
            if os.path.exists(session_file):
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        verification_results = json.load(f)
                    print(f"ğŸ“‚ ì´ì „ ì„¸ì…˜ì—ì„œ {len(verification_results)}ê°œ ê²€ì¦ ê²°ê³¼ë¥¼ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.")
                except:
                    verification_results = []
            
            # ì´ë¯¸ ê²€ì¦ëœ IDë“¤
            verified_ids = {r['id'] for r in verification_results}
            
            # ê²€ì¦ ì‹œì‘ (ê·¸ë£¹ë³„ë¡œ ì„ì–´ì„œ ì§„í–‰)
            import random
            df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)  # ëœë¤ ì„ê¸°
            
            for idx, row in df_shuffled.iterrows():
                if row['id'] in verified_ids:
                    continue
                
                try:
                    result = self.classifier.predict(row['title'], row['content'], row['keyword'])
                    
                    print(f"\n{'='*80}")
                    print(f"ğŸ“° ê¸°ì‚¬ {len(verification_results)+1}/{len(df)}")
                    print(f"ID: {row['id']}")
                    print(f"í‚¤ì›Œë“œ: {row['keyword']}")
                    print(f"ê·¸ë£¹: {row['group_name']} ({df_shuffled['group_name'].value_counts()[row['group_name']]}ê°œ ì¤‘)")
                    print(f"ë‚ ì§œ: {row['created_at']}")
                    
                    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
                    print(f"\nğŸ“‹ ì œëª©: {row['title']}")
                    print(f"ğŸ”— URL: {row['url']}")
                    print(f"ğŸ“ ë‚´ìš© ê¸¸ì´: {len(row['content'])}ì")
                    
                    # ì œëª© vs ë³¸ë¬¸ ë¶ˆì¼ì¹˜ ê°ì§€
                    title_content_mismatch = self.detect_title_content_mismatch(row)
                    if title_content_mismatch:
                        print(f"\nâš ï¸  ì œëª©-ë³¸ë¬¸ ë¶ˆì¼ì¹˜ ê°ì§€: {title_content_mismatch}")
                    
                    # MLì´ ë¨¼ì € ì‚¬ìœ  ìƒì„±
                    ml_reason = self.generate_ml_reason(row, result)
                    print(f"\nğŸ¤– ML ì‚¬ìœ : {ml_reason}")
                    
                    print(f"\nğŸ“Š ëª¨ë¸ ì˜ˆì¸¡:")
                    print(f"  ë¶„ë¥˜: {result['classification']}")
                    print(f"  ì‹ ë¢°ë„: {result['confidence']:.3f}")
                    
                    # í™•ë¥  ë¶„í¬ í‘œì‹œ
                    probs = result['probabilities']
                    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)
                    print(f"  í™•ë¥  ë¶„í¬:")
                    for class_name, prob in sorted_probs:
                        bar = "â–ˆ" * int(prob * 20)
                        print(f"    {class_name}: {prob:.3f} {bar}")
                    
                    # ê²€ì¦ ì˜µì…˜
                    print(f"\nğŸš€ ê²€ì¦ ì˜µì…˜:")
                    print(f"  y = ì˜ˆì¸¡ì´ ë§ìŒ (ì›ë³¸ ê¸°ì‚¬ í™•ì¸ í›„)")
                    print(f"  n = ì˜ˆì¸¡ì´ í‹€ë¦¼ (ì›ë³¸ ê¸°ì‚¬ í™•ì¸ í›„ ìˆ˜ì •)")
                    print(f"  s = ê±´ë„ˆë›°ê¸°")
                    print(f"  q = ì¢…ë£Œ (í˜„ì¬ê¹Œì§€ ì €ì¥)")
                    print(f"  u = URL ì—´ê¸°")
                    print(f"  f = ì „ì²´ ë‚´ìš© ë³´ê¸°")
                    
                    while True:
                        user_input = input(f"\nğŸ¤” ì„ íƒ: ").lower().strip()
                        
                        if user_input == 'q':
                            # ì„¸ì…˜ ì €ì¥ í›„ ì¢…ë£Œ
                            self.save_session(verification_results, session_file)
                            print("ğŸ’¾ ì„¸ì…˜ì„ ì €ì¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
                            return
                        elif user_input == 'u' and row['url']:
                            try:
                                webbrowser.open(row['url'])
                                print("ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ URLì„ ì—´ì—ˆìŠµë‹ˆë‹¤.")
                            except:
                                print("âŒ URLì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            continue
                        elif user_input == 'f':
                            print(f"\nğŸ“„ ì „ì²´ ë‚´ìš©:")
                            print(f"{'='*80}")
                            print(f"{row['content']}")
                            print(f"{'='*80}")
                            continue
                        elif user_input in ['y', 'n', 's']:
                            break
                        else:
                            print("y, n, s, q, u, f ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    
                    if user_input == 's':
                        print("ê±´ë„ˆëœ€")
                        continue
                    
                    # ê²€ì¦ ê²°ê³¼ ì €ì¥
                    verification_result = {
                        'id': row['id'],
                        'title': row['title'],
                        'content': row['content'],
                        'keyword': row['keyword'],
                        'group_name': row['group_name'],
                        'url': row['url'],
                        'created_at': row['created_at'],
                        'predicted_class': result['classification'],
                        'confidence': result['confidence'],
                        'probabilities': result['probabilities'],
                        'ml_reason': ml_reason,
                        'is_correct': user_input == 'y',
                        'verified_at': datetime.now().isoformat()
                    }
                    
                    if user_input == 'n':
                        # í‹€ë¦° ê²½ìš° ë¶„ë¥˜ì™€ ì‚¬ìœ  ìˆ˜ì •
                        final_classification, final_reason = self.modify_classification_and_reason(
                            row, result, ml_reason
                        )
                        verification_result['correct_label'] = final_classification
                        verification_result['correct_reason'] = final_reason
                        verification_result['confidence'] = 1.0  # ìˆ˜ë™ ìˆ˜ì •ì´ë¯€ë¡œ 1.0
                        print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {final_classification} - {final_reason}")
                    else:
                        # ë§ëŠ” ê²½ìš° ML ì‚¬ìœ  ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        verification_result['correct_label'] = result['classification']
                        verification_result['correct_reason'] = ml_reason
                        # MLì˜ confidence_score ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    
                    verification_results.append(verification_result)
                    
                    # ì¦‰ì‹œ DBì— ì €ì¥
                    self.save_single_to_database(verification_result)
                    
                    # ì„¸ì…˜ ìë™ ì €ì¥
                    self.save_session(verification_results, session_file)
                    
                except Exception as e:
                    logger.error(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ID: {row['id']}): {e}")
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            self.save_final_results(verification_results)
            
            # í†µê³„ ì¶œë ¥
            self.show_statistics(verification_results)
            
            # ì„¸ì…˜ íŒŒì¼ ì •ë¦¬
            if os.path.exists(session_file):
                os.remove(session_file)
                print("ğŸ§¹ ì„¸ì…˜ íŒŒì¼ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"í–¥ìƒëœ ì˜ˆì¸¡ ë° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_session(self, results, session_file):
        """ì„¸ì…˜ ì €ì¥"""
        try:
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_final_results(self, results):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"verification_results_{timestamp}.json"
        filepath = os.path.join(self.model_path, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ìµœì¢… ê²€ì¦ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def show_statistics(self, results):
        """í†µê³„ ì¶œë ¥"""
        if not results:
            return
        
        correct_count = sum(1 for r in results if r['is_correct'])
        total_count = len(results)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼:")
        print(f"  ì´ ê²€ì¦: {total_count}ê°œ")
        print(f"  ì •í™•: {correct_count}ê°œ")
        print(f"  ë¶€ì •í™•: {total_count - correct_count}ê°œ")
        print(f"  ì •í™•ë„: {correct_count/total_count*100:.1f}%")
        
        # ë¶„ë¥˜ë³„ í†µê³„
        print(f"\nğŸ“ˆ ë¶„ë¥˜ë³„ í†µê³„:")
        class_stats = {}
        for r in results:
            pred_class = r['predicted_class']
            if pred_class not in class_stats:
                class_stats[pred_class] = {'total': 0, 'correct': 0}
            class_stats[pred_class]['total'] += 1
            if r['is_correct']:
                class_stats[pred_class]['correct'] += 1
        
        for class_name, stats in class_stats.items():
            accuracy = stats['correct'] / stats['total'] * 100
            print(f"  {class_name}: {stats['correct']}/{stats['total']} ({accuracy:.1f}%)")
        
        # ê·¸ë£¹ë³„ í†µê³„
        print(f"\nğŸ“Š ê·¸ë£¹ë³„ í†µê³„:")
        group_stats = {}
        for r in results:
            group_name = r['group_name']
            if group_name not in group_stats:
                group_stats[group_name] = {'total': 0, 'correct': 0}
            group_stats[group_name]['total'] += 1
            if r['is_correct']:
                group_stats[group_name]['correct'] += 1
        
        for group_name, stats in group_stats.items():
            accuracy = stats['correct'] / stats['total'] * 100
            print(f"  {group_name}: {stats['correct']}/{stats['total']} ({accuracy:.1f}%)")
    
    def detect_title_content_mismatch(self, row):
        """ì œëª©ê³¼ ë³¸ë¬¸ì˜ ë¶ˆì¼ì¹˜ë¥¼ ê°ì§€"""
        title = row['title'].lower()
        content = row['content'].lower()
        keyword = row['keyword'].lower()
        
        # ì œëª©ì— í‚¤ì›Œë“œê°€ ì—†ì§€ë§Œ ë³¸ë¬¸ì— ìˆëŠ” ê²½ìš°
        if keyword not in title and keyword in content:
            # ì œëª©ì´ ì¼ë°˜ì ì¸ íŒ¨í„´ì¸ì§€ í™•ì¸
            general_title_patterns = [
                'ê²½ì œê³„ ì¸ì‚¬', 'ì¸ì‚¬', 'ë¶€ì„', 'ì„ëª…', 'ì´ì‚¬', 'ëŒ€í‘œ',
                'ì‹œì¥ ë™í–¥', 'ì—…ê³„ ë™í–¥', 'íŠ¸ë Œë“œ', 'ë¶„ì„',
                'ì£¼ì‹', 'íˆ¬ì', 'ì¦ì‹œ', 'ì½”ìŠ¤í”¼',
                'ë‰´ìŠ¤', 'ì†Œì‹', 'ì „ë§', 'ì „ë§'
            ]
            
            for pattern in general_title_patterns:
                if pattern in title:
                    return f"ì œëª©ì€ '{pattern}'ì´ì§€ë§Œ ë³¸ë¬¸ì— '{keyword}' ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë¨"
        
        # ì œëª©ì— í‚¤ì›Œë“œê°€ ìˆì§€ë§Œ ë³¸ë¬¸ì— ë³´ë„ìë£Œ íŠ¹ì„±ì´ ìˆëŠ” ê²½ìš°
        if keyword in title:
            press_release_indicators = [
                '~ë°í˜”ë‹¤', '~ì „í–ˆë‹¤', '~ë°œí‘œí–ˆë‹¤', '~ì†Œê°œí–ˆë‹¤',
                'ê´€ê³„ìì— ì˜í•˜ë©´', 'ë¸Œëœë“œ ê´€ê³„ì ì¸¡ì€', 'ê´€ê³„ìëŠ”'
            ]
            
            for indicator in press_release_indicators:
                if indicator in content:
                    return f"ì œëª©ì— '{keyword}'ê°€ ìˆì§€ë§Œ ë³¸ë¬¸ì— ë³´ë„ìë£Œ íŠ¹ì„±('{indicator}') ë°œê²¬"
        
        # ì—°ì˜ˆì¸/ì¸í”Œë£¨ì–¸ì„œ ì œí’ˆ ì°©ìš© ê°ì§€
        celebrity_patterns = [
            'ì°¨ì€ìš°', 'ì•„ì´ìœ ', 'ë‰´ì§„ìŠ¤', 'ë¸”ë™í•‘í¬', 'ë°©íƒ„ì†Œë…„ë‹¨', 'ì„¸ë¸í‹´',
            'ì—°ì˜ˆì¸', 'ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ', 'ì¸í”Œë£¨ì–¸ì„œ', 'ìœ íŠœë²„',
            'ìŠ¤íƒ€', 'ì…€ëŸ½', 'ìœ ëª…ì¸'
        ]
        
        for celebrity in celebrity_patterns:
            if celebrity in content and keyword in content:
                return f"ì—°ì˜ˆì¸/ì¸í”Œë£¨ì–¸ì„œ('{celebrity}')ê°€ '{keyword}' ì œí’ˆì„ ì°©ìš©í•˜ëŠ” ë‚´ìš© í¬í•¨"
        
        # ëª¨ê¸°ì—…-ë¸Œëœë“œ ê´€ê³„ ê°ì§€ (í•œê¸€-ì˜ë¬¸ ë³€í˜• í¬í•¨)
        company_brand_relations = {
            "F&F": ["F&F", "ì—í”„ì•¤ì—í”„", "ì—í”„ì•¤ì—í”„í™€ë”©ìŠ¤"],
            "ì´ëœë“œ": ["ì´ëœë“œ", "E-LAND", "ì´ëœë“œì›”ë“œ"],
            "ì˜ì›ë¬´ì—­": ["ì˜ì›ë¬´ì—­", "ì˜ì›ë¬´ì—­ê·¸ë£¹"]
        }
        
        for parent_company, variations in company_brand_relations.items():
            # ì œëª©ì— ëª¨ê¸°ì—…ì´ ìˆì§€ë§Œ í‚¤ì›Œë“œëŠ” ë¸Œëœë“œì¸ ê²½ìš°
            if any(variation in title for variation in variations):
                # ë¸Œëœë“œ ì†Œìœ  ê´€ê³„ í™•ì¸
                for brand_company, ownership_info in self.brand_ownership.items():
                    if brand_company == parent_company:
                        owned_brands = ownership_info["brands"]
                        if keyword in owned_brands:
                            return f"ì œëª©ì— ëª¨ê¸°ì—… '{parent_company}'ì´ ìˆì§€ë§Œ í‚¤ì›Œë“œëŠ” ì†Œìœ  ë¸Œëœë“œ '{keyword}'ì„ (ëª¨ê¸°ì—… ë³´ë„ìë£Œ ê°€ëŠ¥ì„±)"
        
        return None
    
    def generate_ml_reason(self, row, result):
        """MLì´ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ìƒì„±"""
        classification = result['classification']
        confidence = result['confidence']
        title = row['title']
        content = row['content']
        keyword = row['keyword']
        
        # ì œëª©-ë³¸ë¬¸ ë¶ˆì¼ì¹˜ ê°ì§€
        mismatch = self.detect_title_content_mismatch(row)
        
        # ê¸°ë³¸ ì‚¬ìœ  í…œí”Œë¦¿
        reasons = {
            "ë³´ë„ìë£Œ": [
                f"ì œëª©ì— '{keyword}' ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆê³ , ë³¸ë¬¸ì— ë³´ë„ìë£Œ íŠ¹ì„± í‘œí˜„ì´ ë°œê²¬ë¨",
                f"'{keyword}' ê¸°ì—…ì˜ ê³µì‹ ë°œí‘œë‚˜ í™ë³´ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨ë¨",
                f"ê¸°ì—… ê´€ê³„ì ì–¸ê¸‰ê³¼ í•¨ê»˜ '{keyword}' ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë¨"
            ],
            "ì˜¤ê°€ë‹‰": [
                f"ì–¸ë¡ ì‚¬ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ì„±í•œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ë¡œ íŒë‹¨ë¨",
                f"ê°ê´€ì  ë¶„ì„ì´ë‚˜ ì‹œì¥ ë™í–¥ì— '{keyword}'ê°€ ì–¸ê¸‰ë¨",
                f"í™ë³´ì„±ì´ ì•„ë‹Œ ì •ë³´ì„± ë‚´ìš©ì— '{keyword}'ê°€ í¬í•¨ë¨"
            ],
            "í•´ë‹¹ì—†ìŒ": [
                f"ì œëª©ê³¼ ë‚´ìš©ì—ì„œ '{keyword}'ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì£¼ì œë¡œ íŒë‹¨ë¨",
                f"'{keyword}'ê°€ ë‹¨ìˆœíˆ ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ ì‹¤ì œ ë‚´ìš©ì€ ë‹¤ë¥¸ ì£¼ì œ",
                f"ë‹¤ë¥¸ ê¸°ì—…ì˜ ì´ë²¤íŠ¸ì—ì„œ '{keyword}'ê°€ ì°¸ì—¬ ë¸Œëœë“œë¡œë§Œ ì–¸ê¸‰ë¨"
            ]
        }
        
        # ë¶ˆì¼ì¹˜ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìœ  ìˆ˜ì •
        if mismatch:
            if classification == "í•´ë‹¹ì—†ìŒ" and keyword in content.lower():
                # ì—°ì˜ˆì¸ ì°©ìš© ì¼€ì´ìŠ¤ í™•ì¸
                celebrity_found = None
                celebrity_patterns = ['ì°¨ì€ìš°', 'ì•„ì´ìœ ', 'ë‰´ì§„ìŠ¤', 'ë¸”ë™í•‘í¬', 'ë°©íƒ„ì†Œë…„ë‹¨', 'ì„¸ë¸í‹´', 'ì—°ì˜ˆì¸', 'ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ', 'ì¸í”Œë£¨ì–¸ì„œ', 'ìœ íŠœë²„', 'ìŠ¤íƒ€', 'ì…€ëŸ½', 'ìœ ëª…ì¸']
                for celebrity in celebrity_patterns:
                    if celebrity in content.lower():
                        celebrity_found = celebrity
                        break
                
                # ëª¨ê¸°ì—…-ë¸Œëœë“œ ê´€ê³„ ì¼€ì´ìŠ¤ í™•ì¸
                company_brand_relations = {
                    "F&F": ["F&F", "ì—í”„ì•¤ì—í”„", "ì—í”„ì•¤ì—í”„í™€ë”©ìŠ¤"],
                    "ì´ëœë“œ": ["ì´ëœë“œ", "E-LAND", "ì´ëœë“œì›”ë“œ"],
                    "ì˜ì›ë¬´ì—­": ["ì˜ì›ë¬´ì—­", "ì˜ì›ë¬´ì—­ê·¸ë£¹"]
                }
                
                parent_company_found = None
                for parent_company, variations in company_brand_relations.items():
                    if any(variation in title.lower() for variation in variations):
                        # ë¸Œëœë“œ ì†Œìœ  ê´€ê³„ í™•ì¸
                        for brand_company, ownership_info in self.brand_ownership.items():
                            if brand_company == parent_company:
                                owned_brands = ownership_info["brands"]
                                if keyword in owned_brands:
                                    parent_company_found = parent_company
                                    break
                        if parent_company_found:
                            break
                
                if celebrity_found:
                    reasons["ì˜¤ê°€ë‹‰"] = [
                        f"ì—°ì˜ˆì¸/ì¸í”Œë£¨ì–¸ì„œ('{celebrity_found}')ê°€ '{keyword}' ì œí’ˆì„ ì°©ìš©í•˜ëŠ” ë‚´ìš©ì´ í¬í•¨ëœ ì˜¤ê°€ë‹‰ ê¸°ì‚¬",
                        f"íŠ¸ë Œë“œ/ë¼ì´í”„ìŠ¤íƒ€ì¼ ê¸°ì‚¬ì— '{keyword}' ì œí’ˆ ì°©ìš© ë‚´ìš© í¬í•¨",
                        f"ì—°ì˜ˆì¸ ê´€ë ¨ ê¸°ì‚¬ì— '{keyword}' ë¸Œëœë“œ ì œí’ˆì´ ì–¸ê¸‰ë¨"
                    ]
                elif parent_company_found:
                    reasons["í•´ë‹¹ì—†ìŒ"] = [
                        f"ì œëª©ì— ëª¨ê¸°ì—… '{parent_company_found}'ì´ ìˆì§€ë§Œ í‚¤ì›Œë“œëŠ” ì†Œìœ  ë¸Œëœë“œ '{keyword}'ì„ (ëª¨ê¸°ì—… ë³´ë„ìë£Œ ê°€ëŠ¥ì„±)",
                        f"ëª¨ê¸°ì—… ê´€ë ¨ ê¸°ì‚¬ì´ë¯€ë¡œ ë¸Œëœë“œ í‚¤ì›Œë“œì™€ëŠ” ì§ì ‘ì  ê´€ë ¨ ì—†ìŒ",
                        f"ì œëª©ê³¼ í‚¤ì›Œë“œê°€ ë‹¤ë¥¸ ì£¼ì²´ë¥¼ ê°€ë¦¬í‚´ (ëª¨ê¸°ì—… vs ë¸Œëœë“œ)"
                    ]
                else:
                    reasons["í•´ë‹¹ì—†ìŒ"] = [
                        f"ì œëª©ì€ ì¼ë°˜ì ì´ì§€ë§Œ ë³¸ë¬¸ì— '{keyword}' ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë¨ (ì¬ê²€í†  í•„ìš”)",
                        f"ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì´ ë‹¤ë¦„ - ë³¸ë¬¸ í™•ì¸ í•„ìš”",
                        f"ì œëª©ë§Œìœ¼ë¡œëŠ” íŒë‹¨ ì–´ë ¤ì›€ - ë³¸ë¬¸ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ í•„ìš”"
                    ]
            elif classification == "ë³´ë„ìë£Œ":
                reasons["ë³´ë„ìë£Œ"] = [
                    f"ì œëª©ì€ ì¼ë°˜ì ì´ì§€ë§Œ ë³¸ë¬¸ì— '{keyword}' ë³´ë„ìë£Œ íŠ¹ì„± ë°œê²¬",
                    f"ë³¸ë¬¸ì— ê¸°ì—… ê´€ê³„ì ì–¸ê¸‰ê³¼ í•¨ê»˜ '{keyword}' ê´€ë ¨ ì •ë³´ í¬í•¨",
                    f"ì œëª©ê³¼ ë‹¬ë¦¬ ë³¸ë¬¸ì€ '{keyword}' ê¸°ì—…ì˜ ê³µì‹ ë°œí‘œ ë‚´ìš©"
                ]
        
        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì‚¬ìœ  ì„ íƒ
        if confidence > 0.7:
            reason = reasons[classification][0]
        elif confidence > 0.5:
            reason = reasons[classification][1]
        else:
            reason = reasons[classification][2]
        
        return reason
    
    def modify_classification_and_reason(self, row, result, ml_reason):
        """ë¶„ë¥˜ì™€ ì‚¬ìœ ë¥¼ ìˆ˜ì •"""
        print(f"\nğŸ“ ë¶„ë¥˜ì™€ ì‚¬ìœ ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:")
        print(f"í˜„ì¬ ML ì˜ˆì¸¡: {result['classification']} - {ml_reason}")
        
        # ë¶„ë¥˜ ì„ íƒ
        print(f"\nğŸ”¸ ë¶„ë¥˜ ì„ íƒ:")
        for i, category in enumerate(self.classification_guide.keys(), 1):
            print(f"{i}. {category}")
        
        while True:
            try:
                label_input = int(input("ë¶„ë¥˜ ì„ íƒ (1/2/3): ").strip())
                if label_input in [1, 2, 3]:
                    break
                print("1, 2, ë˜ëŠ” 3ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        label_map = {1: 'ë³´ë„ìë£Œ', 2: 'ì˜¤ê°€ë‹‰', 3: 'í•´ë‹¹ì—†ìŒ'}
        final_classification = label_map[label_input]
        
        # ì‚¬ìœ  ìˆ˜ì •
        print(f"\nğŸ“ ì‚¬ìœ  ìˆ˜ì •:")
        print(f"í˜„ì¬ ì‚¬ìœ : {ml_reason}")
        print(f"ìƒˆë¡œìš´ ì‚¬ìœ ë¥¼ ì…ë ¥í•˜ì„¸ìš” (Enterë¡œ í˜„ì¬ ì‚¬ìœ  ìœ ì§€):")
        
        new_reason = input("ì‚¬ìœ : ").strip()
        if not new_reason:
            new_reason = ml_reason
        
        return final_classification, new_reason
    
    def save_single_to_database(self, verification_result):
        """ê°œë³„ ê²€ì¦ ê²°ê³¼ë¥¼ ì¦‰ì‹œ DBì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # classification_logs í…Œì´ë¸”ì— ì €ì¥ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
            cursor.execute("""
                INSERT OR REPLACE INTO classification_logs 
                (url, title, content, keyword, group_name, classification_result, reason, 
                 confidence_score, created_at, is_saved)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                verification_result['url'],
                verification_result['title'],
                verification_result['content'],
                verification_result['keyword'],
                verification_result['group_name'],
                verification_result['correct_label'],
                verification_result['correct_reason'],
                verification_result['confidence'],
                verification_result['verified_at'],
                True  # ìˆ˜ë™ ê²€ì¦ ì™„ë£Œ í‘œì‹œ
            ))
            
            conn.commit()
            conn.close()
            
            confidence_display = f"{verification_result['confidence']:.3f}"
            if verification_result['confidence'] == 1.0:
                confidence_display = "1.000 (ìˆ˜ë™ìˆ˜ì •)"
            print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: {verification_result['correct_label']} (ì‹ ë¢°ë„: {confidence_display}) - {verification_result['correct_reason'][:50]}...")
            
        except Exception as e:
            logger.error(f"ê°œë³„ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_to_database(self, verification_results):
        """ê²€ì¦ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ - í˜¸í™˜ì„± ìœ ì§€)"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for result in verification_results:
                # classification_logs í…Œì´ë¸”ì— ì €ì¥ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
                cursor.execute("""
                    INSERT OR REPLACE INTO classification_logs 
                    (url, title, content, keyword, group_name, classification_result, reason, 
                     confidence_score, created_at, is_saved)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    result['url'],
                    result['title'],
                    result['content'],
                    result['keyword'],
                    result['group_name'],
                    result['correct_label'],
                    result['correct_reason'],
                    result['confidence'],
                    result['verified_at'],
                    True  # ìˆ˜ë™ ê²€ì¦ ì™„ë£Œ í‘œì‹œ
                ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ’¾ {len(verification_results)}ê°œ ê²€ì¦ ê²°ê³¼ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_wrong_predictions_from_verification(self, verification_file):
        """ê²€ì¦ ê²°ê³¼ì—ì„œ í‹€ë¦° ì˜ˆì¸¡ë“¤ ì¶”ì¶œ"""
        try:
            with open(verification_file, 'r', encoding='utf-8') as f:
                verification_data = json.load(f)
            
            # í‹€ë¦° ì˜ˆì¸¡ë“¤ë§Œ í•„í„°ë§
            wrong_predictions = []
            for item in verification_data:
                if not item['is_correct'] and 'correct_label' in item:
                    wrong_predictions.append({
                        'title': item['title'],
                        'content': item['content'],
                        'keyword': item['keyword'],
                        'classification_result': item['correct_label']
                    })
            
            logger.info(f"ğŸ“Š ê²€ì¦ ê²°ê³¼ì—ì„œ í‹€ë¦° ì˜ˆì¸¡ {len(wrong_predictions)}ê°œ ì¶”ì¶œ")
            return wrong_predictions
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== í–¥ìƒëœ ìˆ˜ë™ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘ ===")
    
    verifier = EnhancedVerification()
    
    # ì„¤ì • ì…ë ¥
    try:
        limit = int(input("ê²€ì¦í•  ë°ì´í„° ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 20): ") or "20")
    except ValueError:
        limit = 20
    
    auto_open_input = input("URLì„ ìë™ìœ¼ë¡œ ì—´ê¹Œìš”? (y/n, ê¸°ë³¸ê°’: n): ").lower().strip()
    auto_open = auto_open_input == 'y' if auto_open_input else False
    
    if auto_open:
        print("âš ï¸  ìë™ URL ì—´ê¸°ëŠ” ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆ˜ë™ ê²€ì¦ì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.")
        confirm = input("ì •ë§ ìë™ìœ¼ë¡œ ì—´ê¹Œìš”? (y/n): ").lower().strip()
        if confirm != 'y':
            auto_open = False
            print("âœ… ìˆ˜ë™ ëª¨ë“œë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ’¡ ê²€ì¦ íŒ:")
    print(f"  1. ì›ë³¸ ê¸°ì‚¬ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš” (URL í´ë¦­)")
    print(f"  2. ì œëª©ë§Œìœ¼ë¡œëŠ” ì •í™•í•œ ë¶„ë¥˜ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print(f"  3. ê¸°ì‚¬ ë‚´ìš©ì„ ì½ê³  ë¶„ë¥˜ ê¸°ì¤€ì— ë”°ë¼ íŒë‹¨í•˜ì„¸ìš”")
    print(f"  4. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ 's'ë¡œ ê±´ë„ˆë›°ì„¸ìš”")
    print(f"  5. 'c' ì˜µì…˜ìœ¼ë¡œ URLì„ ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print(f"  6. 'f' ì˜µì…˜ìœ¼ë¡œ ì „ì²´ ë‚´ìš©ì„ í„°ë¯¸ë„ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print(f"  7. ê° ê¸°ì‚¬ë§ˆë‹¤ y/n/s/q ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
    print(f"\n" + "="*80)
    
    # í–¥ìƒëœ ì˜ˆì¸¡ ë° ê²€ì¦ ì‹¤í–‰
    verifier.predict_and_verify_enhanced(limit=limit, auto_open_url=auto_open)
    
    logger.info("=== í–¥ìƒëœ ìˆ˜ë™ ê²€ì¦ ì™„ë£Œ ===")

if __name__ == "__main__":
    main() 