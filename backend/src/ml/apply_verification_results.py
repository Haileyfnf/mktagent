import os
import sqlite3
import pandas as pd
import logging
import json
from datetime import datetime
from news_classifier import NewsClassifier
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VerificationResultProcessor:
    """
    ìˆ˜ë™ ê²€ì¦ ê²°ê³¼ë¥¼ ëª¨ë¸ íŒŒì¸íŠœë‹ì— ì ìš©í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, db_path=None, model_path="models"):
        self.db_path = db_path or os.getenv('DB_PATH')
        self.model_path = model_path
        self.classifier = NewsClassifier(db_path=db_path, model_path=model_path)
    
    def load_verification_results(self, verification_file):
        """ê²€ì¦ ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(verification_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            logger.info(f"ğŸ“‚ ê²€ì¦ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(results)}ê°œ")
            return results
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_wrong_predictions(self, verification_results):
        """í‹€ë¦° ì˜ˆì¸¡ë“¤ë§Œ ì¶”ì¶œ"""
        wrong_predictions = []
        
        for item in verification_results:
            if not item['is_correct'] and 'correct_label' in item:
                wrong_predictions.append({
                    'title': item['title'],
                    'content': item['content'],
                    'keyword': item['keyword'],
                    'classification_result': item['correct_label'],
                    'original_prediction': item['predicted_class'],
                    'confidence': item['confidence'],
                    'group_name': item['group_name']
                })
        
        logger.info(f"ğŸ“Š í‹€ë¦° ì˜ˆì¸¡ {len(wrong_predictions)}ê°œ ì¶”ì¶œ")
        return wrong_predictions
    
    def load_existing_training_data(self):
        """ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            query = """
            SELECT title, content, keyword, classification_result
            FROM articles 
            WHERE classification_result IS NOT NULL
            AND classification_result IN ('ë³´ë„ìë£Œ', 'ì˜¤ê°€ë‹‰', 'í•´ë‹¹ì—†ìŒ')
            """
            
            df = pd.read_sql_query(query, conn)
            conn.close()
            
            logger.info(f"ğŸ“‚ ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
            return df
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
    
    def create_finetuning_dataset(self, wrong_predictions, existing_data):
        """íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        # í‹€ë¦° ì˜ˆì¸¡ë“¤ì„ DataFrameìœ¼ë¡œ ë³€í™˜
        wrong_df = pd.DataFrame(wrong_predictions)
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
        combined_df = pd.concat([existing_data, wrong_df], ignore_index=True)
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€ì´ ì•„ë‹ˆë¼ë©´ ì œëª©+ë‚´ìš© ê¸°ì¤€)
        combined_df = combined_df.drop_duplicates(subset=['title', 'content'], keep='first')
        
        logger.info(f"ğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:")
        logger.info(f"  - ê¸°ì¡´ ë°ì´í„°: {len(existing_data)}ê°œ")
        logger.info(f"  - í‹€ë¦° ì˜ˆì¸¡: {len(wrong_predictions)}ê°œ")
        logger.info(f"  - ìµœì¢… ë°ì´í„°ì…‹: {len(combined_df)}ê°œ")
        
        return combined_df
    
    def save_finetuning_data(self, dataset, filename=None):
        """íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"finetuning_data_{timestamp}.json"
        
        filepath = os.path.join(self.model_path, filename)
        
        try:
            # DataFrameì„ JSONìœ¼ë¡œ ë³€í™˜
            data_list = dataset.to_dict('records')
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data_list, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def run_finetuning(self, finetuning_data_path, epochs=3):
        """ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘...")
            
            # íŒŒì¸íŠœë‹ ë°ì´í„° ë¡œë“œ
            with open(finetuning_data_path, 'r', encoding='utf-8') as f:
                finetuning_data = json.load(f)
            
            # NewsClassifierì˜ íŒŒì¸íŠœë‹ ë©”ì„œë“œ í˜¸ì¶œ
            # (ê¸°ì¡´ train_koelectra_model ë©”ì„œë“œë¥¼ ì¬ì‚¬ìš©)
            success = self.classifier.train_koelectra_model(
                training_data=finetuning_data,
                epochs=epochs,
                is_finetuning=True
            )
            
            if success:
                logger.info("âœ… ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
                return True
            else:
                logger.error("âŒ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤íŒ¨!")
                return False
                
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_improvements(self, verification_results):
        """ê°œì„  ì‚¬í•­ ë¶„ì„"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ ê°œì„  ì‚¬í•­ ë¶„ì„")
        print(f"{'='*80}")
        
        # ê·¸ë£¹ë³„ ì˜¤ë¥˜ ë¶„ì„
        group_errors = {}
        for item in verification_results:
            if not item['is_correct']:
                group = item['group_name']
                if group not in group_errors:
                    group_errors[group] = []
                group_errors[group].append({
                    'predicted': item['predicted_class'],
                    'correct': item['correct_label'],
                    'confidence': item['confidence']
                })
        
        print(f"\nğŸ” ê·¸ë£¹ë³„ ì˜¤ë¥˜ íŒ¨í„´:")
        for group, errors in group_errors.items():
            print(f"\n  {group}:")
            for error in errors:
                print(f"    ì˜ˆì¸¡: {error['predicted']} â†’ ì‹¤ì œ: {error['correct']} (ì‹ ë¢°ë„: {error['confidence']:.3f})")
        
        # ë¶„ë¥˜ë³„ ì˜¤ë¥˜ ë¶„ì„
        class_errors = {}
        for item in verification_results:
            if not item['is_correct']:
                pred_class = item['predicted_class']
                if pred_class not in class_errors:
                    class_errors[pred_class] = []
                class_errors[pred_class].append(item['correct_label'])
        
        print(f"\nğŸ“Š ë¶„ë¥˜ë³„ ì˜¤ë¥˜ íŒ¨í„´:")
        for pred_class, correct_labels in class_errors.items():
            print(f"  {pred_class}ë¡œ ì˜ëª» ë¶„ë¥˜ëœ ê²ƒë“¤:")
            for correct_label in correct_labels:
                print(f"    â†’ {correct_label}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== ê²€ì¦ ê²°ê³¼ íŒŒì¸íŠœë‹ ì ìš© ì‹œì‘ ===")
    
    processor = VerificationResultProcessor()
    
    # 1. ê²€ì¦ ê²°ê³¼ íŒŒì¼ ì—¬ëŸ¬ ê°œ ì„ íƒ
    model_dir = processor.model_path
    verification_files = [f for f in os.listdir(model_dir) if f.startswith('verification_results_')]
    
    if not verification_files:
        logger.error("âŒ ê²€ì¦ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        logger.info("ğŸ’¡ ë¨¼ì € enhanced_verification.pyë¥¼ ì‹¤í–‰í•´ì„œ ê²€ì¦ì„ ì™„ë£Œí•˜ì„¸ìš”.")
        return
    
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ì¦ ê²°ê³¼ íŒŒì¼ ëª©ë¡:")
    for i, fname in enumerate(sorted(verification_files), 1):
        print(f"{i}. {fname}")
    idxs = input("ì‚¬ìš©í•  íŒŒì¼ ë²ˆí˜¸ë¥¼ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3): ")
    idx_list = [int(x.strip())-1 for x in idxs.split(',') if x.strip().isdigit()]
    selected_files = [os.path.join(model_dir, sorted(verification_files)[i]) for i in idx_list if 0 <= i < len(verification_files)]
    if not selected_files:
        print("ì„ íƒëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    print("ğŸ“‚ ì‚¬ìš©í•  ê²€ì¦ ê²°ê³¼ íŒŒì¼:")
    for f in selected_files:
        print(" -", os.path.basename(f))
    # ì—¬ëŸ¬ íŒŒì¼ì˜ ê²°ê³¼ í•©ì¹˜ê¸°
    all_verification_results = []
    for f in selected_files:
        results = processor.load_verification_results(f)
        if results:
            all_verification_results.extend(results)
    verification_results = all_verification_results
    if not verification_results:
        print("ì„ íƒí•œ íŒŒì¼ì—ì„œ ê²€ì¦ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 3. í‹€ë¦° ì˜ˆì¸¡ ì¶”ì¶œ
    wrong_predictions = processor.extract_wrong_predictions(verification_results)
    if not wrong_predictions:
        logger.warning("âš ï¸ í‹€ë¦° ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¸íŠœë‹ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # 4. ê°œì„  ì‚¬í•­ ë¶„ì„
    processor.analyze_improvements(verification_results)
    
    # 5. ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ
    existing_data = processor.load_existing_training_data()
    
    # 6. íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„±
    finetuning_dataset = processor.create_finetuning_dataset(wrong_predictions, existing_data)

    # íŒŒì¸íŠœë‹ ë°ì´í„° ë¶„í¬ í™•ì¸ ë° ê²½ê³ 
    counts = finetuning_dataset['classification_result'].value_counts()
    print("\n[íŒŒì¸íŠœë‹ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬]")
    print(counts)
    if (counts < 2).any():
        print("âš ï¸ ê° í´ë˜ìŠ¤ë³„ë¡œ ìµœì†Œ 2ê°œ ì´ìƒ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìˆ˜ë™ ê²€ì¦ ë°ì´í„°ë¥¼ ë” ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
        return
    
    # 7. íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥
    finetuning_data_path = processor.save_finetuning_data(finetuning_dataset)
    if not finetuning_data_path:
        return
    
    # 8. ì‚¬ìš©ì í™•ì¸
    print(f"\n{'='*80}")
    print(f"ğŸš€ íŒŒì¸íŠœë‹ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"  - í‹€ë¦° ì˜ˆì¸¡: {len(wrong_predictions)}ê°œ")
    print(f"  - ê¸°ì¡´ ë°ì´í„°: {len(existing_data)}ê°œ")
    print(f"  - ìµœì¢… ë°ì´í„°ì…‹: {len(finetuning_dataset)}ê°œ")
    print(f"  - íŒŒì¸íŠœë‹ ë°ì´í„°: {finetuning_data_path}")
    
    confirm = input(f"\nğŸ¤” íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
    
    if confirm == 'y':
        # 9. íŒŒì¸íŠœë‹ ì‹¤í–‰
        epochs = int(input("íŒŒì¸íŠœë‹ ì—í¬í¬ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 3): ") or "3")
        
        success = processor.run_finetuning(finetuning_data_path, epochs=epochs)
        
        if success:
            print(f"\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
            print(f"ğŸ’¡ ì´ì œ ìƒˆë¡œìš´ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")
        else:
            print(f"\nâŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨!")
    else:
        print(f"\nğŸ’¡ íŒŒì¸íŠœë‹ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   íŒŒì¸íŠœë‹ ë°ì´í„°ëŠ” {finetuning_data_path}ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    logger.info("=== ê²€ì¦ ê²°ê³¼ íŒŒì¸íŠœë‹ ì ìš© ì™„ë£Œ ===")

if __name__ == "__main__":
    main() 