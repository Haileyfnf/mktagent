import os
import re
import html
import sqlite3
import time
from urllib.parse import urlparse, quote
from flask import Blueprint, request, jsonify
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================

# ë„¤ì´ë²„ API ì„¤ì •
NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET')

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'database', 'db.sqlite'))

# Flask Blueprint ì„¤ì •
naver_news_bp = Blueprint('naver_news', __name__)

# ë””ë²„ê¹…ì„ ìœ„í•œ DB ê²½ë¡œ ì¶œë ¥
print(f"ğŸ” DB ê²½ë¡œ: {DB_PATH}")
print(f"ğŸ” DB íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(DB_PATH)}")

DOMAIN_PRESS_MAP = {
    "joins.com": "ì¤‘ì•™ì¼ë³´",
    "chosun.com": "ì¡°ì„ ì¼ë³´",
    "hani.co.kr": "í•œê²¨ë ˆ",
    "donga.com": "ë™ì•„ì¼ë³´",
    "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
    # ... í•„ìš”ì‹œ ì¶”ê°€
}

# ============================================================================
# ë°ì´í„° ì •ì œ í•¨ìˆ˜ë“¤
# ============================================================================

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸ ì œê±°, NBSP ì œê±°, ê³µë°± ì •ë¦¬, psp ì œê±°, ZWNBSP ì œê±°)"""
    if not text:
        return ''
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<.*?>', '', text)
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = html.unescape(text)
    # NBSP ë° íŠ¹ìˆ˜ ê³µë°± ë¬¸ì ì œê±°
    text = text.replace('&nbsp;', ' ').replace('\xa0', ' ')
    text = re.sub(r'[\u00A0\u200B\u200C\u200D\uFEFF]', ' ', text)
    # ZWNBSP(Zero Width No-Break Space) ì œê±°
    text = text.replace('\uFEFF', '')
    # ZWNBSP(Zero Width No-Break Space) ì¶”ê°€ ì œê±°
    text = text.replace('\u2060', '')  # WORD JOINER
    text = text.replace('\u200B', '')  # ZERO WIDTH SPACE
    text = text.replace('\u200C', '')  # ZERO WIDTH NON-JOINER
    text = text.replace('\u200D', '')  # ZERO WIDTH JOINER
    # psp(ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´) ëª¨ë‘ ì œê±°
    text = re.sub(r'psp', '', text, flags=re.IGNORECASE)
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def clean_press_domain(press):
    """ì–¸ë¡ ì‚¬ ë„ë©”ì¸ì—ì„œ í™•ì¥ì ì œê±°"""
    if not press:
        return press
    
    # www. ì œê±°
    press = press.replace('www.', '')
    
    # .co.kr, .com, .net ë“± ì œê±°
    press = re.sub(r'\.(co\.kr|com|net|org|kr)$', '', press)
    press = re.sub(r'\.(info|biz|edu|gov|mil|int)$', '', press)
    
    return press.strip()

def format_date(date_str):
    """ë‚ ì§œ í˜•ì‹ì„ yyyy-mm-dd HH:MM:SSë¡œ ë³€í™˜"""
    if not date_str:
        return date_str
    
    try:
        # RFC 2822 í˜•ì‹ íŒŒì‹± (ì˜ˆ: "Mon, 08 Jul 2025 13:34:51 +0900")
        date_formats = [
            "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
            "%Y-%m-%d %H:%M:%S",         # ISO í˜•ì‹
            "%Y-%m-%d",                  # ë‚ ì§œë§Œ
            "%d %b %Y %H:%M:%S",         # ì‹œê°„ëŒ€ ì—†ìŒ
        ]
        
        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                return parsed_date.strftime("%Y-%m-%d %H:%M:%S")
            except ValueError:
                continue
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return date_str
        
    except Exception as e:
        print(f"ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {date_str}, ì˜¤ë¥˜: {e}")
        return date_str

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤
# ============================================================================

def get_active_keywords_from_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™œì„±í™”ëœ í‚¤ì›Œë“œë“¤ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("SELECT keyword, group_name FROM keywords WHERE is_active = 1")
    keywords_data = cursor.fetchall()
    
    conn.close()
    return keywords_data

# ============================================================================
# ê¸°ì‚¬ ì¶”ì¶œ ë° ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def extract_press_from_url(originallink):
    """URLì—ì„œ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì¶”ì¶œ"""
    if not originallink:
        return ""
    domain = urlparse(originallink).netloc
    return domain

def extract_article_content(url):
    """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if 'news.naver.com' in url:
            content_selectors = [
                '#articleBody',
                '#articleBodyContents',
                '.article_body',
                '#content',
                '.news_end',
            ]
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    for unwanted in content_element.select('script, style, .reporter_area, .copyright, .link_news'):
                        unwanted.decompose()
                    content = content_element.get_text(strip=True)
                    if content and len(content) > 100:
                        return clean_text(content)
        
        general_selectors = [
            'article',
            '.article-content',
            '.news-content',
            '.content',
            '.post-content',
            '.entry-content',
            'main',
            '.main-content'
        ]
        for selector in general_selectors:
            content_elements = soup.select(selector)
            for element in content_elements:
                for unwanted in element.select('script, style, nav, header, footer, .advertisement, .sidebar'):
                    unwanted.decompose()
                content = element.get_text(strip=True)
                if content and len(content) > 200:
                    return clean_text(content)
        
        body = soup.find('body')
        if body:
            for unwanted in body.select('script, style, nav, header, footer, .advertisement, .sidebar, .comment'):
                unwanted.decompose()
            content = body.get_text(strip=True)
            if content and len(content) > 300:
                return clean_text(content)
        
        return ""
    except Exception:
        return ""

def save_article_to_db(article, keyword):
    """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì •ì œ í¬í•¨)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # ê·¸ë£¹ëª… ì¡°íšŒ
    cursor.execute("SELECT group_name FROM keywords WHERE keyword=? AND is_active=1", (keyword,))
    row = cursor.fetchone()
    group_name = row[0] if row else None
    
    # ì¤‘ë³µ URL ê²€ì‚¬
    cursor.execute("SELECT id FROM articles WHERE url = ?", (article['link'],))
    if cursor.fetchone():
        conn.close()
        return False
    
    # ì›ë³¸ ë°ì´í„° ì¶”ì¶œ
    title = clean_text(article.get('title', ''))
    content = extract_article_content(article.get('link', ''))
    press = extract_press_from_url(article.get('originallink', ''))
    pub_date = article.get('pubDate', '')
    
    # ë°ì´í„° ì •ì œ
    content = clean_text(content)
    press = clean_press_domain(press)
    pub_date = format_date(pub_date)
    
    # ì œëª©ì´ ëª¨ë‘ ì˜ë¬¸ì¸ì§€ í™•ì¸ (í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì œì™¸)
    if title and not re.search(r'[ê°€-í£]', title):
        # ì˜ë¬¸, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ”ì§€ í™•ì¸
        if re.search(r'[A-Za-z]', title):
            print(f"ğŸ”¤ [í•„í„°] ì˜ë¬¸ ì œëª© ê¸°ì‚¬ ì œì™¸: {title[:40]} ...")
            conn.close()
            return False
    
    # ì•¼êµ¬ ê´€ë ¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    baseball_terms = [
        'ì´ì •í›„', 'ê¹€í•˜ì„±', 'ì´ë‹', 'ì‹¤ì ', 'íˆ¬ìˆ˜', 'íƒ€ì', 'í™ˆëŸ°', 'ì•¼êµ¬', 'KBO', 'ì‚¼ì§„', 'íƒ€ìœ¨', 'ë„ë£¨',
        'í¬ìˆ˜', 'ë§ˆìš´ë“œ', 'ê²½ê¸°', 'ì„ ë°œ', 'ë¶ˆíœ', 'íƒ€ì ', 'ë“ì ', 'ì•ˆíƒ€', 'ë³¼ë„·', 'ìŠ¤íŠ¸ë¼ì´í¬',
        'í¬ìŠ¤íŠ¸ì‹œì¦Œ', 'ì›”ë“œì‹œë¦¬ì¦ˆ', 'ë©”ì´ì €ë¦¬ê·¸', 'MLB', 'êµ¬ë‹¨', 'ê°ë…', 'ì½”ì¹˜', 'ì„ ìˆ˜', 'íƒ€ìˆœ',
        'íƒ€ê²©', 'ìˆ˜ë¹„', 'ì—°ë´‰', 'ì´ì ', 'íŠ¸ë ˆì´ë“œ', 'ì‹œë²”ê²½ê¸°', 'í”Œë ˆì´ì˜¤í”„', 'í´ë¦°ì—…', 'ì‚¬êµ¬', 'í™ˆí”Œë ˆì´íŠ¸',
        'ì™¸ì•¼ìˆ˜', 'ë‚´ì•¼ìˆ˜', 'ì£¼ë£¨', 'ìŠ¬ë¼ì´ë”©', 'ì‚¬ì´ë“œì•”', 'ì–¸ë”í•¸ë“œ', 'ì¢Œì™„', 'ìš°ì™„', 'ì™„íˆ¬', 'ì™„ë´‰', 'ë…¸íˆíŠ¸', 'ë…¸ëŸ°'
    ]

    # MLB/ì— ì—˜ë¹„ + ì•¼êµ¬ ê¸°ì‚¬ í•„í„°ë§
    if keyword in ['MLB', 'ì— ì—˜ë¹„']:
        if any(term in title for term in baseball_terms) or any(term in content for term in baseball_terms):
            print(f"âš¾ï¸ [í•„í„°] MLB/ì— ì—˜ë¹„ í‚¤ì›Œë“œ ì•¼êµ¬ ê¸°ì‚¬ ì œì™¸: {title[:40]} ...")
            conn.close()
            return False
    
    # F&F í‚¤ì›Œë“œ íŠ¹ë³„ í•„í„°ë§
    if keyword == 'F&F':
        # 1. ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— ì‹¤ì œë¡œ F&Fê°€ ì—†ëŠ” ê¸°ì‚¬ í•„í„°ë§ (ë„¤ì´ë²„ API ì˜¤íƒì§€ ë°©ì§€)
        if 'F&F' not in title and 'F&F' not in content and 'f&f' not in title.lower() and 'f&f' not in content.lower():
            print(f"ğŸ” [í•„í„°] F&F í‚¤ì›Œë“œ ì˜¤íƒì§€ ê¸°ì‚¬ ì œì™¸: {title[:40]} ... (ì œëª©/ë³¸ë¬¸ì— F&F ì—†ìŒ)")
            conn.close()
            return False
        
        # 2. ì§€ì£¼ì‚¬ ê´€ë ¨ì£¼ ë‚˜ì—´ ê¸°ì‚¬ í•„í„°ë§
        if '(ì§€ì£¼ì‚¬ ê´€ë ¨ì£¼)' in content or 'ì§€ì£¼ì‚¬ ê´€ë ¨ì£¼' in content:
            # í™€ë”©ìŠ¤/ì§€ì£¼ ê´€ë ¨ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸
            holding_companies = [
                'í™€ë”©ìŠ¤', 'ì§€ì£¼', 'ëŒ€ìƒí™€ë”©ìŠ¤', 'í•œí™”', 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼', 'GRT', 'í•œì§„ì¤‘ê³µì—…í™€ë”©ìŠ¤', 
                'ë¡œìŠ¤ì›°', 'ì„±ì°½ê¸°ì—…ì§€ì£¼', 'í‰í™”í™€ë”©ìŠ¤', 'BNKê¸ˆìœµì§€ì£¼', 'ìš°ë¦¬ì‚°ì—…í™€ë”©ìŠ¤', 'íœ´ë§¥ìŠ¤í™€ë”©ìŠ¤',
                'ë¹„ì¸ ë¡œí…Œí¬', 'ë„¤ì˜¤ìœ„ì¦ˆí™€ë”©ìŠ¤', 'ë¶€ë°©', 'í•œêµ­ì½œë§ˆí™€ë”©ìŠ¤', 'ë””ì™€ì´', 'í•œë¯¸ì‚¬ì´ì–¸ìŠ¤',
                'LSì „ì„ ì•„ì‹œì•„', 'ì»´íˆ¬ìŠ¤í™€ë”©ìŠ¤', 'JBê¸ˆìœµì§€ì£¼', 'ì†”ë³¸', 'ê¸€ë¡œë²Œì—ìŠ¤ì— ', 'ì—˜ë¸Œì´ì— ì”¨í™€ë”©ìŠ¤',
                'KBê¸ˆìœµ', 'DGBê¸ˆìœµì§€ì£¼', 'ìŠˆí”„ë¦¬ë§ˆì—ì´ì¹˜í', 'KCê·¸ë¦°í™€ë”©ìŠ¤', 'CNH', 'BGF', 'í’€ë¬´ì›',
                'ì¼ë™í™€ë”©ìŠ¤', 'ì‹ ì†¡í™€ë”©ìŠ¤', 'ì˜¤ê°€ë‹‰í‹°ì½”ìŠ¤ë©”í‹±', 'ë…¹ì‹­ìí™€ë”©ìŠ¤', 'ì‹ í•œì§€ì£¼', 'ìš°ë¦¬ê¸ˆìœµì§€ì£¼',
                'APS', 'íœ´ì˜¨ìŠ¤ê¸€ë¡œë²Œ', 'ë•ì‚°í•˜ì´ë©”íƒˆ', 'ì´ì§€í™€ë”©ìŠ¤', 'ì¼ì§„í™€ë”©ìŠ¤', 'ìœ™ì…í‘¸ë“œ', 'ì˜¤ë¦¬ì˜¨í™€ë”©ìŠ¤',
                'CRí™€ë”©ìŠ¤', 'SKë””ìŠ¤ì»¤ë²„ë¦¬', 'ì½”ì•„ì‹œì•„', 'DRBë™ì¼', 'ê³¨ë“ ì„¼ì¸„ë¦¬', 'ì›…ì§„', 'ë¡¯ë°ì§€ì£¼',
                'ì½”ì˜¤ë¡±', 'ë™êµ­í™€ë”©ìŠ¤', 'ë©”ë¦¬ì¸ ê¸ˆìœµì§€ì£¼', 'í•´ì„±ì‚°ì—…', 'ì œì¼íŒŒë§ˆí™€ë”©ìŠ¤', 'LG', 'AJë„¤íŠ¸ì›ìŠ¤',
                'HDC', 'ì—ì½”í”„ë¡œ', 'ê²½ë™ì¸ë² ìŠ¤íŠ¸', 'GS', 'SJMí™€ë”©ìŠ¤', 'ìœ ìˆ˜í™€ë”©ìŠ¤', 'ì„œì—°',
                'ìœ ë¹„ì¿¼ìŠ¤í™€ë”©ìŠ¤', 'ìƒ˜í‘œ', 'ì‚¼ì„±ë¬¼ì‚°', 'ì´ê±´í™€ë”©ìŠ¤', 'ì´ë…¹ìŠ¤', 'ê¸ˆí˜¸ê±´ì„¤', 'ë™ì•„ì˜ì‹œì˜¤í™€ë”©ìŠ¤',
                'ëŒ€ë•', 'ì•„ì„¸ì•„', 'LXí™€ë”©ìŠ¤', 'ëŒ€ì›…', 'ì†”ë¸Œë ˆì¸í™€ë”©ìŠ¤', 'ë™ì„±ì¼€ë¯¸ì»¬', 'íš¨ì„±', 'HDí˜„ëŒ€',
                'í’ì‚°í™€ë”©ìŠ¤', 'NICE', 'ì‚¼ì–‘í™€ë”©ìŠ¤', 'SKìŠ¤í€˜ì–´', 'í•œì„¸ì˜ˆìŠ¤24í™€ë”©ìŠ¤', 'KPXí™€ë”©ìŠ¤',
                'í•œì†”í™€ë”©ìŠ¤', 'ê·¸ë˜ë””ì–¸íŠ¸', 'ì‹¬í…í™€ë”©ìŠ¤', 'CSí™€ë”©ìŠ¤', 'F&Fí™€ë”©ìŠ¤', 'ì˜ì›ë¬´ì—­í™€ë”©ìŠ¤',
                'ê³¨í”„ì¡´ë‰´ë”˜í™€ë”©ìŠ¤', 'í•˜ì´íŠ¸ì§„ë¡œí™€ë”©ìŠ¤', 'ë…¸ë£¨í™€ë”©ìŠ¤', 'KISCOí™€ë”©ìŠ¤', 'AKí™€ë”©ìŠ¤', 'DB',
                'ì˜ˆìŠ¤ì½”í™€ë”©ìŠ¤', 'ì½”ìŠ¤ë§¥ìŠ¤ë¹„í‹°ì•„ì´', 'ì•„ì´ë””ìŠ¤í™€ë”©ìŠ¤', 'ë†ì‹¬í™€ë”©ìŠ¤', 'ì§„ì–‘í™€ë”©ìŠ¤', 'ë‘ì‚°ë°¥ìº£', 'í—ì…ê·¸ë£¹'
            ]
            
            # ë³¸ë¬¸ì—ì„œ í™€ë”©ìŠ¤/ì§€ì£¼ ê´€ë ¨ ê¸°ì—…ëª…ì´ 5ê°œ ì´ìƒ ì–¸ê¸‰ë˜ë©´ í•„í„°ë§
            holding_count = sum(1 for company in holding_companies if company in content)
            if holding_count >= 5:
                print(f"ğŸ¢ [í•„í„°] F&F í‚¤ì›Œë“œ ì§€ì£¼ì‚¬ ê´€ë ¨ì£¼ ë‚˜ì—´ ê¸°ì‚¬ ì œì™¸: {title[:40]} ... (í™€ë”©ìŠ¤ ê´€ë ¨ ê¸°ì—… {holding_count}ê°œ ì–¸ê¸‰)")
                conn.close()
                return False
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    cursor.execute("""
        INSERT INTO articles (keyword, group_name, title, content, press, pub_date, url)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (
        keyword,
        group_name,
        title,
        content,
        press,
        pub_date,
        article.get('link', '')
    ))
    conn.commit()
    conn.close()
    return True

# ============================================================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ API ê´€ë ¨ í•¨ìˆ˜ë“¤
# ============================================================================

def search_naver_news(keyword, display=10, start=1, sort='date', start_date=None, end_date=None):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API ê²€ìƒ‰"""
    url = 'https://openapi.naver.com/v1/search/news.json'
    headers = {
        'X-Naver-Client-Id': NAVER_CLIENT_ID,
        'X-Naver-Client-Secret': NAVER_CLIENT_SECRET
    }
    
    search_query_encoded = quote(keyword, safe='')
    
    params = {
        'query': search_query_encoded,
        'display': display,
        'start': start,
        'sort': sort
    }
    
    print(f"[DEBUG] ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì¿¼ë¦¬: {keyword} â†’ {search_query_encoded}")
    response = requests.get(url, headers=headers, params=params)
    result = response.json()
    
    # ë‚ ì§œ í•„í„°ë§ (í•„ìš”ì‹œ)
    if start_date and end_date and 'items' in result:
        filtered_items = []
        for article in result['items']:
            pub_date_str = article.get('pubDate', '')
            if pub_date_str:
                try:
                    pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %z')
                    pub_date_local = pub_date.replace(tzinfo=None)
                    if start_date <= pub_date_local <= end_date:
                        filtered_items.append(article)
                except ValueError:
                    continue
        result['items'] = filtered_items
        result['total'] = len(filtered_items)
    
    return result

def search_naver_news_all_pages(keyword, start_date=None, end_date=None):
    """í˜ì´ì§€ ëê¹Œì§€ ìµœëŒ€ 100ê°œ ê¸°ì‚¬ë§Œ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    all_articles = []
    page = 1
    display = 100  # í•œ ë²ˆì— 100ê°œì”© ê°€ì ¸ì˜¤ê¸°

    print(f"    ğŸ” í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
    print(f"    ğŸ“„ í˜ì´ì§€ë³„ ê²€ìƒ‰ ì‹œì‘...")

    while len(all_articles) < 100:
        start_index = (page - 1) * display + 1
        try:
            result = search_naver_news(keyword, display=display, start=start_index, sort='date', 
                                     start_date=start_date, end_date=end_date)
            articles = result.get('items', [])
            if not articles:
                print(f"    ğŸ“„ í˜ì´ì§€ {page}: ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ)")
                break
            # ë‚¨ì€ ìˆ˜ë§Œí¼ë§Œ ì¶”ê°€
            remain = 100 - len(all_articles)
            all_articles.extend(articles[:remain])
            print(f"    ğŸ“„ í˜ì´ì§€ {page}: {len(articles[:remain])}ê°œ ê¸°ì‚¬ ê²€ìƒ‰ (start={start_index})")
            time.sleep(0.1)
            page += 1
            if len(all_articles) >= 100:
                print(f"    âš ï¸ ìµœëŒ€ ìˆ˜ì§‘ í•œë„(100ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break
        except Exception as e:
            print(f"    âŒ í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            break
    print(f"    ğŸ“Š í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ê²€ìƒ‰")
    return all_articles

def filter_and_save_articles(articles, keyword):
    """ê¸°ì‚¬ë“¤ì„ í•„í„°ë§í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    filtered_articles = []
    saved_count = 0
    skipped_count = 0
    duplicate_count = 0
    
    for article in articles:
        title = clean_text(article.get('title', ''))
        url = article.get('link', '')
        
        # URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT id FROM articles WHERE url = ?", (url,))
        is_duplicate = cursor.fetchone() is not None
        conn.close()
        
        if is_duplicate:
            duplicate_count += 1
            filtered_articles.append({
                'title': title,
                'url': url,
                'status': 'duplicate',
                'reason': 'URL ì¤‘ë³µ'
            })
            continue
        
        # ë³¸ë¬¸ ì¶”ì¶œ ë° DB ì €ì¥
        if save_article_to_db(article, keyword):
            saved_count += 1
            filtered_articles.append({
                'title': title,
                'url': url,
                'status': 'saved'
            })
        else:
            skipped_count += 1
    
    return {
        'saved_count': saved_count,
        'skipped_count': skipped_count,
        'duplicate_count': duplicate_count,
        'filtered_articles': filtered_articles
    }

# ============================================================================
# Flask API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================================================

@naver_news_bp.route('/news/search', methods=['GET'])
def news_search():
    """ë‰´ìŠ¤ ê²€ìƒ‰ API"""
    keyword = request.args.get('keyword')
    display = int(request.args.get('display', 10))
    start = int(request.args.get('start', 1))
    sort = request.args.get('sort', 'date')
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    
    if not keyword:
        return jsonify({'error': 'í‚¤ì›Œë“œ í•„ìš”'}), 400
    
    parsed_start_date = None
    parsed_end_date = None
    if start_date and end_date:
        try:
            parsed_start_date = datetime.strptime(start_date, '%Y-%m-%d')
            parsed_end_date = datetime.strptime(end_date, '%Y-%m-%d')
        except ValueError:
            return jsonify({'error': 'ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ (YYYY-MM-DD)'}), 400
    
    result = search_naver_news(keyword, display=display, start=start, sort=sort, 
                              start_date=parsed_start_date, end_date=parsed_end_date)
    return jsonify(result)

@naver_news_bp.route('/news/search_all_pages', methods=['POST'])
def search_all_pages():
    """í˜ì´ì§€ ëê¹Œì§€ ëª¨ë“  ê¸°ì‚¬ ê²€ìƒ‰ ë° ì €ì¥"""
    data = request.get_json()
    keyword = data.get('keyword')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    
    if not keyword:
        return jsonify({'error': 'í‚¤ì›Œë“œ í•„ìš”'}), 400
    
    parsed_start_date = None
    parsed_end_date = None
    if start_date and end_date:
        try:
            parsed_start_date = datetime.strptime(start_date, '%Y-%m-%d')
            parsed_end_date = datetime.strptime(end_date, '%Y-%m-%d')
        except ValueError:
            return jsonify({'error': 'ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ (YYYY-MM-DD)'}), 400
    
    # í˜ì´ì§€ ëê¹Œì§€ ëª¨ë“  ê¸°ì‚¬ ê²€ìƒ‰
    articles = search_naver_news_all_pages(keyword, start_date=parsed_start_date, end_date=parsed_end_date)
    
    # í•„í„°ë§ ë° ì €ì¥
    filter_result = filter_and_save_articles(articles, keyword)
    return jsonify({
        'message': f'{filter_result["saved_count"]}ê±´ ì €ì¥ ì™„ë£Œ', 
        'total': len(articles), 
        'saved': filter_result["saved_count"],
        'skipped': filter_result["skipped_count"],
        'duplicate': filter_result["duplicate_count"],
        'filtered_articles': filter_result["filtered_articles"],
        'date_range': f"{start_date} ~ {end_date}" if start_date and end_date else "ì „ì²´ ê¸°ê°„"
    })

# ============================================================================
# ì •ì‹ ì—…ë¬´ ìˆ˜í–‰ í•¨ìˆ˜ë“¤
# ============================================================================

def run_news_collection():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì—…ë¬´ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œ)"""
    print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—…ë¬´ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {
            'success': False,
            'error': 'ë„¤ì´ë²„ API í‚¤ ë¯¸ì„¤ì •',
            'total_articles': 0,
            'saved_articles': 0
        }
    try:
        active_keywords_data = get_active_keywords_from_db()
        if not active_keywords_data:
            print("âŒ í™œì„±í™”ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'success': False,
                'error': 'í™œì„±í™”ëœ í‚¤ì›Œë“œ ì—†ìŒ',
                'total_articles': 0,
                'saved_articles': 0
            }
        active_keywords = [row[0] for row in active_keywords_data]
        print(f"ğŸ“‹ ì²˜ë¦¬í•  í‚¤ì›Œë“œ: {active_keywords}")
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'success': False,
            'error': f'í‚¤ì›Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}',
            'total_articles': 0,
            'saved_articles': 0
        }
    total_articles = 0
    saved_articles = 0
    failed_keywords = []
    for keyword in active_keywords:
        print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘...")
        try:
            articles = search_naver_news_all_pages(keyword)
            print(f"  ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬")
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            for article in articles:
                url = article.get('link', '')
                cursor.execute("SELECT id FROM articles WHERE url = ?", (url,))
                if cursor.fetchone() is not None:
                    print(f"  âš ï¸ ì¤‘ë³µ URL ë°œê²¬: {url} â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
                    break
                # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì €ì¥
                if save_article_to_db(article, keyword):
                    saved_articles += 1
            conn.close()
            total_articles += len(articles)
        except Exception as e:
            print(f"  âŒ í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            failed_keywords.append(keyword)
    print(f"\nğŸ“ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  ì´ ê²€ìƒ‰ëœ ê¸°ì‚¬: {total_articles}ê°œ")
    print(f"  ì´ ì €ì¥ëœ ê¸°ì‚¬: {saved_articles}ê°œ")
    print(f"  ì €ì¥ ì„±ê³µë¥ : {(saved_articles/total_articles*100):.1f}%" if total_articles > 0 else "  ì €ì¥ ì„±ê³µë¥ : 0%")
    if failed_keywords:
        print(f"  ì‹¤íŒ¨í•œ í‚¤ì›Œë“œ: {failed_keywords}")
    return {
        'success': True,
        'total_articles': total_articles,
        'saved_articles': saved_articles,
        'failed_keywords': failed_keywords,
        'success_rate': (saved_articles/total_articles*100) if total_articles > 0 else 0
    }

def run_news_collection_for_keyword(keyword, start_date=None, end_date=None):
    """íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰"""
    print(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        # í˜ì´ì§€ ëê¹Œì§€ ëª¨ë“  ê¸°ì‚¬ ê²€ìƒ‰
        articles = search_naver_news_all_pages(keyword, start_date=start_date, end_date=end_date)
        
        if not articles:
            print(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'success': True,
                'keyword': keyword,
                'total_articles': 0,
                'saved_articles': 0
            }
        
        # í•„í„°ë§ ë° ì €ì¥
        filter_result = filter_and_save_articles(articles, keyword)
        
        print(f"âœ… í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì™„ë£Œ")
        print(f"  ê²€ìƒ‰ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
        print(f"  ì €ì¥ëœ ê¸°ì‚¬: {filter_result['saved_count']}ê°œ")
        
        return {
            'success': True,
            'keyword': keyword,
            'total_articles': len(articles),
            'saved_articles': filter_result['saved_count'],
            'skipped_count': filter_result['skipped_count'],
            'duplicate_count': filter_result['duplicate_count']
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'success': False,
            'keyword': keyword,
            'error': str(e),
            'total_articles': 0,
            'saved_articles': 0
        }

# ============================================================================
# ì‹¤í–‰ ì½”ë“œ (í…ŒìŠ¤íŠ¸ ë° ì§ì ‘ ì‹¤í–‰ìš©)
# ============================================================================

if __name__ == "__main__":
    # ì •ì‹ ì—…ë¬´ ì‹¤í–‰
    result = run_news_collection()
    
    if result['success']:
        print(f"\nğŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—…ë¬´ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"   ì €ì¥ ì„±ê³µë¥ : {result['success_rate']:.1f}%")
    else:
        print(f"\nâš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—…ë¬´ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result['error']}")
    
    print("âœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ") 